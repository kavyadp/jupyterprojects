{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d04e734-6c0a-44a4-b477-2c398640b412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scgpt\n",
      "  Downloading scgpt-0.2.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting cell-gears<0.0.3 (from scgpt)\n",
      "  Downloading cell-gears-0.0.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets<3.0.0,>=2.3.0 (from scgpt)\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: leidenalg>=0.8.10 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scgpt) (0.10.2)\n",
      "Requirement already satisfied: numba>=0.55.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scgpt) (0.60.0)\n",
      "Collecting orbax<0.1.8 (from scgpt)\n",
      "  Downloading orbax-0.1.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scgpt) (2.2.3)\n",
      "Requirement already satisfied: scanpy<2.0.0,>=1.9.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scgpt) (1.10.3)\n",
      "Collecting scib<2.0.0,>=1.0.3 (from scgpt)\n",
      "  Downloading scib-1.1.7-1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: scikit-misc>=0.1.4 in /home/master/.local/lib/python3.9/site-packages (from scgpt) (0.3.1)\n",
      "Collecting scvi-tools<1.0,>=0.16.0 (from scgpt)\n",
      "  Downloading scvi_tools-0.20.3-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting torch<2.2,>=1.13.0 (from scgpt)\n",
      "  Downloading torch-2.1.2-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchtext (from scgpt)\n",
      "  Downloading torchtext-0.18.0-cp39-cp39-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scgpt) (4.12.2)\n",
      "Requirement already satisfied: umap-learn>=0.5.3 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scgpt) (0.5.7)\n",
      "Requirement already satisfied: numpy in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from cell-gears<0.0.3->scgpt) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from cell-gears<0.0.3->scgpt) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from cell-gears<0.0.3->scgpt) (1.6.1)\n",
      "Requirement already satisfied: networkx in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from cell-gears<0.0.3->scgpt) (3.2.1)\n",
      "Collecting dcor (from cell-gears<0.0.3->scgpt)\n",
      "  Downloading dcor-0.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: filelock in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scgpt) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scgpt) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<3.0.0,>=2.3.0->scgpt)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scgpt) (2.32.3)\n",
      "Collecting xxhash (from datasets<3.0.0,>=2.3.0->scgpt)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets<3.0.0,>=2.3.0->scgpt)\n",
      "  Downloading multiprocess-0.70.17-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0,>=2.3.0->scgpt)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scgpt) (3.11.11)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets<3.0.0,>=2.3.0->scgpt)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scgpt) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scgpt) (6.0.2)\n",
      "Requirement already satisfied: igraph<0.12,>=0.10.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from leidenalg>=0.8.10->scgpt) (0.11.8)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from numba>=0.55.1->scgpt) (0.43.0)\n",
      "Collecting absl-py (from orbax<0.1.8->scgpt)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: cached_property in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scgpt) (1.5.2)\n",
      "Requirement already satisfied: importlib_resources in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scgpt) (6.4.5)\n",
      "Collecting msgpack (from orbax<0.1.8->scgpt)\n",
      "  Downloading msgpack-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting etils (from orbax<0.1.8->scgpt)\n",
      "  Downloading etils-1.5.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jax>=0.4.6 (from orbax<0.1.8->scgpt)\n",
      "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from orbax<0.1.8->scgpt)\n",
      "  Downloading jaxlib-0.4.30-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting tensorstore>=0.1.20 (from orbax<0.1.8->scgpt)\n",
      "  Downloading tensorstore-0.1.69-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: nest_asyncio in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scgpt) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas>=1.3.5->scgpt) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas>=1.3.5->scgpt) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas>=1.3.5->scgpt) (2024.2)\n",
      "Requirement already satisfied: anndata>=0.8 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.10.9)\n",
      "Requirement already satisfied: get-annotations in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.1.2)\n",
      "Requirement already satisfied: h5py>=3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (3.12.1)\n",
      "Requirement already satisfied: joblib in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (1.4.2)\n",
      "Requirement already satisfied: legacy-api-wrap>=1.4 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (3.9.4)\n",
      "Requirement already satisfied: natsort in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (8.4.0)\n",
      "Requirement already satisfied: patsy in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.5.13)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (1.13.1)\n",
      "Requirement already satisfied: seaborn>=0.13 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.13.2)\n",
      "Requirement already satisfied: session-info in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (1.0.0)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.14.4)\n",
      "Collecting pydot (from scib<2.0.0,>=1.0.3->scgpt)\n",
      "  Downloading pydot-3.0.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting deprecated (from scib<2.0.0,>=1.0.3->scgpt)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting chex (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading chex-0.1.88-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting docrep>=0.3.2 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading docrep-0.3.2.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flax (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading flax-0.8.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ml-collections>=0.1.1 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mudata>=0.1.2 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading mudata-0.2.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting numpyro (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading numpyro-0.17.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting openpyxl>=3.0 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting optax (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading optax-0.2.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pyro-ppl>=1.6.0 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting pytorch-lightning<1.10.0,>=1.9.0 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting rich>=12.0.0 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting torchmetrics>=0.11.0 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: sympy in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch<2.2,>=1.13.0->scgpt) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch<2.2,>=1.13.0->scgpt) (3.1.5)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.2,>=1.13.0->scgpt)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "INFO: pip is looking at multiple versions of torchtext to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchtext (from scgpt)\n",
      "  Downloading torchtext-0.17.2-cp39-cp39-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "  Downloading torchtext-0.17.1-cp39-cp39-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
      "  Downloading torchtext-0.17.0-cp39-cp39-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
      "  Downloading torchtext-0.16.2-cp39-cp39-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting torchdata==0.7.1 (from torchtext->scgpt)\n",
      "  Downloading torchdata-0.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torchdata==0.7.1->torchtext->scgpt) (1.26.20)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from anndata>=0.8->scanpy<2.0.0,>=1.9.1->scgpt) (1.10.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from anndata>=0.8->scanpy<2.0.0,>=1.9.1->scgpt) (1.2.2)\n",
      "Requirement already satisfied: six in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from docrep>=0.3.2->scvi-tools<1.0,>=0.16.0->scgpt) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (1.18.3)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from igraph<0.12,>=0.10.0->leidenalg>=0.8.10->scgpt) (1.7.0)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax>=0.4.6->orbax<0.1.8->scgpt)\n",
      "  Downloading ml_dtypes-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting opt-einsum (from jax>=0.4.6->orbax<0.1.8->scgpt)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from jax>=0.4.6->orbax<0.1.8->scgpt) (8.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (3.2.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from importlib_resources->orbax<0.1.8->scgpt) (3.21.0)\n",
      "Collecting contextlib2 (from ml-collections>=0.1.1->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting et-xmlfile (from openpyxl>=3.0->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.6.0->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<1.10.0,>=1.9.0->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.3.0->scgpt) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.3.0->scgpt) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.3.0->scgpt) (2024.12.14)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.0.0->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from rich>=12.0.0->scvi-tools<1.0,>=0.16.0->scgpt) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn->cell-gears<0.0.3->scgpt) (3.5.0)\n",
      "Collecting toolz>=0.9.0 (from chex->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from deprecated->scib<2.0.0,>=1.0.3->scgpt) (1.17.1)\n",
      "Collecting orbax-checkpoint (from flax->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading orbax_checkpoint-0.6.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from jinja2->torch<2.2,>=1.13.0->scgpt) (3.0.2)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets<3.0.0,>=2.3.0->scgpt)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting multipledispatch (from numpyro->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading multipledispatch-1.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: stdlib-list in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from session-info->scanpy<2.0.0,>=1.9.1->scgpt) (0.11.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from sympy->torch<2.2,>=1.13.0->scgpt) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from lightning-utilities>=0.6.0.post0->pytorch-lightning<1.10.0,>=1.9.0->scvi-tools<1.0,>=0.16.0->scgpt) (75.1.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.0.0->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting protobuf (from orbax-checkpoint->flax->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting humanize (from orbax-checkpoint->flax->scvi-tools<1.0,>=0.16.0->scgpt)\n",
      "  Downloading humanize-4.11.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Downloading scgpt-0.2.1-py3-none-any.whl (829 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading orbax-0.1.7-py3-none-any.whl (77 kB)\n",
      "Downloading scib-1.1.7-1-py3-none-any.whl (84 kB)\n",
      "Downloading scvi_tools-0.20.3-py3-none-any.whl (330 kB)\n",
      "Downloading torch-2.1.2-cp39-cp39-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m115.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "Downloading torchtext-0.16.2-cp39-cp39-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading torchdata-0.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.4.30-cp39-cp39-manylinux2014_x86_64.whl (79.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mudata-0.2.4-py3-none-any.whl (24 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading tensorstore-0.1.69-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading chex-0.1.88-py3-none-any.whl (99 kB)\n",
      "Downloading dcor-0.6-py3-none-any.whl (55 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading etils-1.5.2-py3-none-any.whl (140 kB)\n",
      "Downloading flax-0.8.5-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.3/731.3 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (377 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Downloading numpyro-0.17.0-py3-none-any.whl (360 kB)\n",
      "Downloading optax-0.2.4-py3-none-any.whl (319 kB)\n",
      "Downloading pydot-3.0.4-py3-none-any.whl (35 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading ml_dtypes-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.2/39.2 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading orbax_checkpoint-0.6.4-py3-none-any.whl (270 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading humanize-4.11.0-py3-none-any.whl (128 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Building wheels for collected packages: cell-gears, docrep, ml-collections\n",
      "  Building wheel for cell-gears (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cell-gears: filename=cell_gears-0.0.2-py3-none-any.whl size=27809 sha256=b04fda3a642398b4b8603fb3854c9c47a3d117a6ed3c579609eb235ffd7b5b25\n",
      "  Stored in directory: /home/master/.cache/pip/wheels/6e/0e/6f/2d371e9f362f18d985aa303386f613f7496b19fd92ae656d69\n",
      "  Building wheel for docrep (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docrep: filename=docrep-0.3.2-py3-none-any.whl size=19877 sha256=3484d80c132265aeedf7b0151bd390bbd652d19c64286ca21219836d88b92eb4\n",
      "  Stored in directory: /home/master/.cache/pip/wheels/e5/f0/3f/17394a03ed36922f620186a5a9ebd6bce2b4579020243c7a68\n",
      "  Building wheel for ml-collections (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94508 sha256=782ffb262fe1ef6fd26b5d16200f1d7c27675ccc05133604db0195d837daf891\n",
      "  Stored in directory: /home/master/.cache/pip/wheels/fd/c2/0d/5d94d95e5875ea17b85a9f1f99b8dd2e50517137c8042c6468\n",
      "Successfully built cell-gears docrep ml-collections\n",
      "Installing collected packages: pyro-api, multipledispatch, xxhash, triton, toolz, pydot, protobuf, opt-einsum, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgpack, ml-dtypes, mdurl, lightning-utilities, humanize, fsspec, etils, et-xmlfile, docrep, dill, deprecated, contextlib2, absl-py, tensorstore, openpyxl, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, ml-collections, markdown-it-py, jaxlib, huggingface-hub, dcor, rich, nvidia-cusolver-cu12, jax, torch, orbax-checkpoint, orbax, numpyro, mudata, chex, torchmetrics, torchdata, pyro-ppl, optax, datasets, torchtext, scib, pytorch-lightning, flax, cell-gears, scvi-tools, scgpt\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2024.12.0 requires fsspec==2024.12.0.*, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.1.0 cell-gears-0.0.2 chex-0.1.88 contextlib2-21.6.0 datasets-2.21.0 dcor-0.6 deprecated-1.2.18 dill-0.3.8 docrep-0.3.2 et-xmlfile-2.0.0 etils-1.5.2 flax-0.8.5 fsspec-2024.6.1 huggingface-hub-0.28.1 humanize-4.11.0 jax-0.4.30 jaxlib-0.4.30 lightning-utilities-0.12.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-collections-0.1.1 ml-dtypes-0.5.1 msgpack-1.1.0 mudata-0.2.4 multipledispatch-1.0.0 multiprocess-0.70.16 numpyro-0.17.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.8.61 nvidia-nvtx-cu12-12.1.105 openpyxl-3.1.5 opt-einsum-3.4.0 optax-0.2.4 orbax-0.1.7 orbax-checkpoint-0.6.4 protobuf-5.29.3 pydot-3.0.4 pyro-api-0.1.2 pyro-ppl-1.9.1 pytorch-lightning-1.9.5 rich-13.9.4 scgpt-0.2.1 scib-1.1.7 scvi-tools-0.20.3 tensorstore-0.1.69 toolz-1.0.0 torch-2.1.2 torchdata-0.7.1 torchmetrics-1.6.1 torchtext-0.16.2 triton-2.1.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scgpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe9a12b-7a4e-49f8-b6fa-c0cc0d82801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/master/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/master/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'TransformerModel' has no attribute 'load_from_checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pretrained scGPT model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pretrained_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/pretrained/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update with actual path\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m(pretrained_model_path)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'TransformerModel' has no attribute 'load_from_checkpoint'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scgpt.model import TransformerModel\n",
    "\n",
    "# Load the pretrained scGPT model\n",
    "pretrained_model_path = \"path/to/pretrained/model.pt\"  # Update with actual path\n",
    "model = TransformerModel.load_from_checkpoint(pretrained_model_path)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf5ff6c-b091-4836-875b-8314534daa89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (2.1.2)\n",
      "Requirement already satisfied: scanpy in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (1.10.3)\n",
      "Requirement already satisfied: anndata in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (0.10.9)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.61)\n",
      "Requirement already satisfied: get-annotations in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (0.1.2)\n",
      "Requirement already satisfied: h5py>=3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (3.12.1)\n",
      "Requirement already satisfied: joblib in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (1.4.2)\n",
      "Requirement already satisfied: legacy-api-wrap>=1.4 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (3.9.4)\n",
      "Requirement already satisfied: natsort in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (8.4.0)\n",
      "Requirement already satisfied: numba>=0.56 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (1.26.4)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (24.2)\n",
      "Requirement already satisfied: pandas>=1.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (2.2.3)\n",
      "Requirement already satisfied: patsy in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (0.5.13)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (1.13.1)\n",
      "Requirement already satisfied: seaborn>=0.13 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (0.13.2)\n",
      "Requirement already satisfied: session-info in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (1.0.0)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (0.14.4)\n",
      "Requirement already satisfied: tqdm in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (4.67.1)\n",
      "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy) (0.5.7)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from anndata) (1.10.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from anndata) (1.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy) (6.4.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from numba>=0.56->scanpy) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas>=1.5->scanpy) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas>=1.5->scanpy) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: stdlib-list in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from session-info->scanpy) (0.11.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.6->scanpy) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.6->scanpy) (1.17.0)\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 kB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch scanpy anndata transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6461e68a-f7e9-450f-89da-cf39e03ba3da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/bowang-lab/scGPT.git\n",
      "  Cloning https://github.com/bowang-lab/scGPT.git to /tmp/pip-req-build-bcr23onr\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/bowang-lab/scGPT.git /tmp/pip-req-build-bcr23onr\n",
      "  Resolved https://github.com/bowang-lab/scGPT.git to commit 7301b51a72f5db321fccebb51bc4dd1380d99023\n",
      "  Installing build dependencies ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cell-gears<0.0.3 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (0.0.2)\n",
      "Requirement already satisfied: datasets<3.0.0,>=2.3.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (2.21.0)\n",
      "Requirement already satisfied: leidenalg>=0.8.10 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (0.10.2)\n",
      "Requirement already satisfied: numba>=0.55.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (0.60.0)\n",
      "Requirement already satisfied: orbax<0.1.8 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (0.1.7)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (2.2.3)\n",
      "Requirement already satisfied: scanpy<2.0.0,>=1.9.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (1.10.3)\n",
      "Requirement already satisfied: scib<2.0.0,>=1.0.3 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (1.1.7)\n",
      "Requirement already satisfied: scikit-misc>=0.1.4 in /home/master/.local/lib/python3.9/site-packages (from scGPT==0.2.1) (0.3.1)\n",
      "Requirement already satisfied: scvi-tools<1.0,>=0.16.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (0.20.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (2.1.2)\n",
      "Requirement already satisfied: torchtext in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (0.16.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (4.12.2)\n",
      "Requirement already satisfied: umap-learn>=0.5.3 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scGPT==0.2.1) (0.5.7)\n",
      "Requirement already satisfied: numpy in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from cell-gears<0.0.3->scGPT==0.2.1) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from cell-gears<0.0.3->scGPT==0.2.1) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from cell-gears<0.0.3->scGPT==0.2.1) (1.6.1)\n",
      "Requirement already satisfied: networkx in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from cell-gears<0.0.3->scGPT==0.2.1) (3.2.1)\n",
      "Requirement already satisfied: dcor in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from cell-gears<0.0.3->scGPT==0.2.1) (0.6)\n",
      "Requirement already satisfied: filelock in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (0.28.1)\n",
      "Requirement already satisfied: packaging in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (6.0.2)\n",
      "Requirement already satisfied: igraph<0.12,>=0.10.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from leidenalg>=0.8.10->scGPT==0.2.1) (0.11.8)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from numba>=0.55.1->scGPT==0.2.1) (0.43.0)\n",
      "Requirement already satisfied: absl-py in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scGPT==0.2.1) (2.1.0)\n",
      "Requirement already satisfied: cached_property in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scGPT==0.2.1) (1.5.2)\n",
      "Requirement already satisfied: importlib_resources in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scGPT==0.2.1) (6.4.5)\n",
      "Requirement already satisfied: msgpack in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scGPT==0.2.1) (1.1.0)\n",
      "Requirement already satisfied: etils in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scGPT==0.2.1) (1.5.2)\n",
      "Requirement already satisfied: jax>=0.4.6 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scGPT==0.2.1) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scGPT==0.2.1) (0.4.30)\n",
      "Requirement already satisfied: tensorstore>=0.1.20 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scGPT==0.2.1) (0.1.69)\n",
      "Requirement already satisfied: nest_asyncio in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax<0.1.8->scGPT==0.2.1) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas>=1.3.5->scGPT==0.2.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas>=1.3.5->scGPT==0.2.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas>=1.3.5->scGPT==0.2.1) (2024.2)\n",
      "Requirement already satisfied: anndata>=0.8 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (0.10.9)\n",
      "Requirement already satisfied: get-annotations in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (0.1.2)\n",
      "Requirement already satisfied: h5py>=3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (3.12.1)\n",
      "Requirement already satisfied: joblib in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (1.4.2)\n",
      "Requirement already satisfied: legacy-api-wrap>=1.4 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (3.9.4)\n",
      "Requirement already satisfied: natsort in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (8.4.0)\n",
      "Requirement already satisfied: patsy in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (0.5.13)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (1.13.1)\n",
      "Requirement already satisfied: seaborn>=0.13 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (0.13.2)\n",
      "Requirement already satisfied: session-info in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (1.0.0)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (0.14.4)\n",
      "Requirement already satisfied: pydot in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scib<2.0.0,>=1.0.3->scGPT==0.2.1) (3.0.4)\n",
      "Requirement already satisfied: deprecated in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scib<2.0.0,>=1.0.3->scGPT==0.2.1) (1.2.18)\n",
      "Requirement already satisfied: chex in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.1.88)\n",
      "Requirement already satisfied: docrep>=0.3.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.3.2)\n",
      "Requirement already satisfied: flax in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.8.5)\n",
      "Requirement already satisfied: ml-collections>=0.1.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.1.1)\n",
      "Requirement already satisfied: mudata>=0.1.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.2.4)\n",
      "Requirement already satisfied: numpyro in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.17.0)\n",
      "Requirement already satisfied: openpyxl>=3.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (3.1.5)\n",
      "Requirement already satisfied: optax in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.2.4)\n",
      "Requirement already satisfied: pyro-ppl>=1.6.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (1.9.1)\n",
      "Requirement already satisfied: pytorch-lightning<1.10.0,>=1.9.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (1.9.5)\n",
      "Requirement already satisfied: rich>=12.0.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (13.9.4)\n",
      "Requirement already satisfied: torchmetrics>=0.11.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (1.6.1)\n",
      "Requirement already satisfied: sympy in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.13.0->scGPT==0.2.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->scGPT==0.2.1) (12.8.61)\n",
      "Requirement already satisfied: torchdata==0.7.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torchtext->scGPT==0.2.1) (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from torchdata==0.7.1->torchtext->scGPT==0.2.1) (1.26.20)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from anndata>=0.8->scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (1.10.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from anndata>=0.8->scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (1.2.2)\n",
      "Requirement already satisfied: six in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from docrep>=0.3.2->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (1.18.3)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from igraph<0.12,>=0.10.0->leidenalg>=0.8.10->scGPT==0.2.1) (1.7.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from jax>=0.4.6->orbax<0.1.8->scGPT==0.2.1) (0.5.1)\n",
      "Requirement already satisfied: opt-einsum in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from jax>=0.4.6->orbax<0.1.8->scGPT==0.2.1) (3.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from jax>=0.4.6->orbax<0.1.8->scGPT==0.2.1) (8.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (3.2.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from importlib_resources->orbax<0.1.8->scGPT==0.2.1) (3.21.0)\n",
      "Requirement already satisfied: contextlib2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from ml-collections>=0.1.1->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (21.6.0)\n",
      "Requirement already satisfied: et-xmlfile in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from openpyxl>=3.0->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (2.0.0)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pyro-ppl>=1.6.0->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.1.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from pytorch-lightning<1.10.0,>=1.9.0->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.12.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.3.0->scGPT==0.2.1) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from rich>=12.0.0->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from rich>=12.0.0->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn->cell-gears<0.0.3->scGPT==0.2.1) (3.5.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from chex->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (1.0.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from deprecated->scib<2.0.0,>=1.0.3->scGPT==0.2.1) (1.17.1)\n",
      "Requirement already satisfied: orbax-checkpoint in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from flax->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.6.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->scGPT==0.2.1) (3.0.2)\n",
      "Requirement already satisfied: multipledispatch in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from numpyro->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (1.0.0)\n",
      "Requirement already satisfied: stdlib-list in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from session-info->scanpy<2.0.0,>=1.9.1->scGPT==0.2.1) (0.11.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from sympy->torch>=1.13.0->scGPT==0.2.1) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from lightning-utilities>=0.6.0.post0->pytorch-lightning<1.10.0,>=1.9.0->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (75.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (0.1.2)\n",
      "Requirement already satisfied: protobuf in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax-checkpoint->flax->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (5.29.3)\n",
      "Requirement already satisfied: humanize in /home/master/anaconda3/envs/myenv/lib/python3.9/site-packages (from orbax-checkpoint->flax->scvi-tools<1.0,>=0.16.0->scGPT==0.2.1) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/bowang-lab/scGPT.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2a8a4e3-0dc2-4a83-9ac3-f8aa0fc4d953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab keys: ['RP5-973N23.5', 'RP11-182N22.10', 'CTB-53D8.3', 'RP11-348N17.2', 'RP11-205M20.8', 'RP11-326C3.17', 'RP11-439H13.3', 'RP11-413H22.3', 'GET1-SH3BGR', 'CH17-476P10.1']\n",
      "Vocab type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with open(vocab_path, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print(\"Vocab keys:\", list(vocab.keys())[:10])  # Print first 10 vocab entries\n",
    "print(\"Vocab type:\", type(vocab))  # Should be a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b36a0c0-fde0-4cd0-9b7f-01f6635bd74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pad_token index: 60694\n"
     ]
    }
   ],
   "source": [
    "pad_token = model_args.get(\"pad_token\", \"<PAD>\")  # Use default \"<PAD>\" if missing\n",
    "\n",
    "if pad_token not in vocab:\n",
    "    print(f\"⚠️ Warning: pad_token '{pad_token}' not found in vocab!\")\n",
    "    vocab[pad_token] = 0  # Assign default padding index\n",
    "\n",
    "print(f\" pad_token index: {vocab[pad_token]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dfc1023-a6a6-4b4b-ad4b-aca11f66a0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/master/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/master/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming model from /home/master/Documents/scgpt/best_model.pt, overriding configs from /home/master/Documents/scgpt/args.json.\n",
      " Using device: cuda\n",
      " Warning: Some parameters may not match! Loading matching params only.\n",
      " Loading param encoder.enc_norm.weight with shape torch.Size([512])\n",
      " Loading param encoder.enc_norm.bias with shape torch.Size([512])\n",
      " Loading param value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      " Loading param value_encoder.linear1.bias with shape torch.Size([512])\n",
      " Loading param value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param value_encoder.linear2.bias with shape torch.Size([512])\n",
      " Loading param value_encoder.norm.weight with shape torch.Size([512])\n",
      " Loading param value_encoder.norm.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      " Loading param transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      " Loading param transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      " Loading param decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      " Loading param decoder.fc.0.bias with shape torch.Size([512])\n",
      " Loading param decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      " Loading param decoder.fc.2.bias with shape torch.Size([512])\n",
      " Loading param decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      " Loading param decoder.fc.4.bias with shape torch.Size([1])\n",
      " scGPT Pretrained Model Loaded Successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from scgpt.model import TransformerModel\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import GeneVocab\n",
    "\n",
    "# *Define Paths to Model Files**\n",
    "model_dir = Path(\"/home/master/Documents/scgpt\")  # Update with correct path\n",
    "model_config_file = model_dir / \"args.json\"\n",
    "model_file = model_dir / \"best_model.pt\"\n",
    "vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "#  Load Vocabulary Using Official GeneVocab**\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "special_tokens = [\"<PAD>\", \"<CLS>\", \"<MASK>\", \"<UNK>\"]  # Define special tokens if not present\n",
    "\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s)\n",
    "\n",
    "gene2idx = vocab.get_stoi()  # Convert vocab to index mapping\n",
    "\n",
    "#  Load Model Hyperparameters from args.json**\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "\n",
    "print(f\"Resuming model from {model_file}, overriding configs from {model_config_file}.\")\n",
    "\n",
    "# Extract hyperparameters from args.json\n",
    "embsize = model_configs[\"embsize\"]\n",
    "nhead = model_configs[\"nheads\"]\n",
    "d_hid = model_configs[\"d_hid\"]\n",
    "nlayers = model_configs[\"nlayers\"]\n",
    "n_layers_cls = model_configs.get(\"n_layers_cls\", 1)  # Default to 1 if missing\n",
    "pad_value = model_configs.get(\"pad_value\", 0)  # Default padding value\n",
    "n_input_bins = model_configs.get(\"n_input_bins\", 51)  # Default bin count\n",
    "\n",
    "# **Detect GPU & Set Device**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "# ** Initialize scGPT Model with Correct Vocab & Configs**\n",
    "ntokens = len(vocab)  # Size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    pad_value=pad_value,\n",
    "    n_input_bins=n_input_bins,\n",
    ").to(device)\n",
    "\n",
    "# ** Load Pre-trained Weights**\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "    print(f\" Successfully loaded all model parameters from {model_file}\")\n",
    "except Exception as e:\n",
    "    print(f\" Warning: Some parameters may not match! Loading matching params only.\")\n",
    "    \n",
    "    # Load only matching parameters\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_file, map_location=device)\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    for k, v in pretrained_dict.items():\n",
    "        print(f\" Loading param {k} with shape {v.shape}\")\n",
    "    \n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "# ** Move Model to GPU for Faster Inference**\n",
    "model.to(device)\n",
    "\n",
    "print(\" scGPT Pretrained Model Loaded Successfully on\", device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27939cc3-6b8e-48fd-a34f-e80c02a252d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (encoder): GeneEncoder(\n",
      "    (embedding): Embedding(60701, 512, padding_idx=60694)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (value_encoder): ContinuousValueEncoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ExprDecoder(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cls_decoder): ClsDecoder(\n",
      "    (_decoder): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (sim): Similarity(\n",
      "    (cos): CosineSimilarity()\n",
      "  )\n",
      "  (creterion_cce): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb073ea-0838-405f-b59d-325b06f199f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 51,334,146\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Trainable Parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1a939a-4cd5-44aa-a11a-a71794a3acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 1 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 2 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 3 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 4 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 5 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 6 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 7 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 8 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 9 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 10 Attention Weights Shape: torch.Size([1536, 512])\n",
      "Layer 11 Attention Weights Shape: torch.Size([1536, 512])\n"
     ]
    }
   ],
   "source": [
    "# Iterate through transformer layers to print attention weights\n",
    "for i, layer in enumerate(model.transformer_encoder.layers):\n",
    "    attn_weights = layer.self_attn.in_proj_weight  # Attention projection weights\n",
    "    print(f\"Layer {i} Attention Weights Shape: {attn_weights.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12720ad-30ad-4615-9245-14eb8e934130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size in MB: 195.82 MB\n"
     ]
    }
   ],
   "source": [
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f\"Model Size in MB: {param_size / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3651ba72-5608-45f5-876f-916af82aee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU Memory: 23.61 GB\n",
      "Allocated Memory: 0.23 GB\n",
      "Cached Memory: 0.59 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Available GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved() / (1024**3):.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393cde49-85b9-424f-9829-84a74ff3305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of args.json: dict_keys(['data_source', 'save_dir', 'load_model', 'n_hvg', 'valid_size_or_ratio', 'dist_backend', 'grad_accu_steps', 'pad_token', 'input_style', 'input_emb_style', 'n_bins', 'max_seq_len', 'training_tasks', 'dist_url', 'mask_ratio', 'trunc_by_sample', 'vocab_path', 'rank', 'batch_size', 'eval_batch_size', 'epochs', 'lr', 'scheduler_interval', 'scheduler_factor', 'warmup_ratio_or_step', 'no_cls', 'no_cce', 'fp16', 'fast_transformer', 'nlayers', 'nheads', 'embsize', 'd_hid', 'dropout', 'n_layers_cls', 'log_interval', 'save_interval', 'mask_value', 'pad_value', 'USE_CLS', 'USE_CCE', 'MVC', 'USE_GENERATIVE_TRAINING', 'world_size', 'distributed', 'local_rank', 'gpu'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "args_path = \"/home/master/Documents/scgpt/args.json\"\n",
    "\n",
    "# Load model arguments\n",
    "with open(args_path, \"r\") as f:\n",
    "    model_args = json.load(f)\n",
    "\n",
    "# Print to check keys\n",
    "print(\"Contents of args.json:\", model_args.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5cfe18f-3051-4ce5-a97e-a4b669786422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AdversarialDiscriminator', 'Any', 'Bernoulli', 'ClsDecoder', 'ContinuousValueEncoder', 'DomainSpecificBatchNorm1d', 'DomainSpecificBatchNorm2d', 'ExprDecoder', 'F', 'FastTransformerEncoderWrapper', 'FlashTransformerEncoderLayer', 'Function', 'GeneEncoder', 'GradReverse', 'MVCDecoder', 'Mapping', 'MultiOmicTransformerModel', 'Optional', 'PositionalEncoding', 'Similarity', 'Tensor', 'TransformerEncoder', 'TransformerEncoderLayer', 'TransformerGenerator', 'TransformerModel', 'Tuple', 'Union', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'dataset', 'dist', 'dsbn', 'generate_square_subsequent_mask', 'generation_model', 'grad_reverse', 'logger', 'map_raw_id_to_vocab_id', 'math', 'model', 'multiomic_model', 'nn', 'os', 'torch', 'trange']\n"
     ]
    }
   ],
   "source": [
    "import scgpt.model\n",
    "print(dir(scgpt.model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393aaad5-a6f3-47b3-9e5c-49aeb407b344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Single-cell dataset loaded!\n",
      "AnnData object with n_obs × n_vars = 352734 × 45453\n",
      "    obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'harm_study', 'harm_healthy.tissue', 'harm_tumor.site', 'harm_sample.type', 'harm_condition', 'harm_tumor.type', 'harm_cd45pos', 'harm_healthy.pat', 'percent.mt', 'ratio_nCount_nFeature', 'batch', 'X_scvi_batch', 'X_scvi_labels', 'X_scvi_local_l_mean', 'X_scvi_local_l_var', 'leiden_0.2', 'leiden_0.4', 'leiden_0.6', 'leiden_0.8', 'leiden_1', 'leiden_1.2', 'leiden_1.4', 'author_first_cell_type', 'author_cell_type', 'cnv_score', 'organism_ontology_term_id', 'donor_id', 'development_stage_ontology_term_id', 'sex_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'disease_ontology_term_id', 'tissue_ontology_term_id', 'cell_type_ontology_term_id', 'suspension_type', 'assay_ontology_term_id', 'cell_id', 'dup', 'is_primary_data', 'tissue_type', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage', 'observation_joinid'\n",
      "    var: 'features', 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype', 'feature_length', 'feature_type'\n",
      "    uns: 'citation', 'schema_reference', 'schema_version', 'title'\n",
      "    obsm: 'X_umap'\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Define dataset path\n",
    "adata_path = \"/home/master/Documents/scgpt/897e76b2-59f6-482c-827d-37cc62fa4f50.h5ad\"  # Update path if needed\n",
    "\n",
    "# Load dataset\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "\n",
    "# Print dataset summary\n",
    "print(\" Single-cell dataset loaded!\")\n",
    "print(adata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32a10ec4-9f83-4f69-8fc4-d7c0324eece8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in adata.var: Index(['features', 'feature_is_filtered', 'feature_name', 'feature_reference',\n",
      "       'feature_biotype', 'feature_length', 'feature_type'],\n",
      "      dtype='object')\n",
      "First 10 feature names: index\n",
      "ENSG00000000003      TSPAN6\n",
      "ENSG00000000005        TNMD\n",
      "ENSG00000000419        DPM1\n",
      "ENSG00000000457       SCYL3\n",
      "ENSG00000000460    C1orf112\n",
      "ENSG00000000938         FGR\n",
      "ENSG00000000971         CFH\n",
      "ENSG00000001036       FUCA2\n",
      "ENSG00000001084        GCLC\n",
      "ENSG00000001167        NFYA\n",
      "Name: feature_name, dtype: category\n",
      "Categories (45453, object): ['7SK_ENSG00000202198', 'A1BG', 'A1BG-AS1', 'A1CF', ..., 'ZZEF1', 'ZZZ3', 'hsa-mir-1253', 'hsa-mir-423']\n"
     ]
    }
   ],
   "source": [
    "# Check available columns in `adata.var`\n",
    "print(\"Available columns in adata.var:\", adata.var.keys())\n",
    "\n",
    "# Check the first few feature names\n",
    "print(\"First 10 feature names:\", adata.var[\"feature_name\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e500134-80cc-418f-942d-a1a03878a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def tokenize_gene_expression(adata, vocab):\n",
    "    \"\"\"\n",
    "    Convert gene expression data to tokenized form using scGPT vocabulary.\n",
    "    \"\"\"\n",
    "    gene_names = adata.var[\"feature_name\"].tolist()  # Extract gene names\n",
    "    stoi = vocab.get_stoi()  # scGPT's String-To-Index mapping\n",
    "\n",
    "    tokenized_data = []\n",
    "    for cell in adata.X:\n",
    "        tokens = [stoi.get(gene, stoi.get(\"<UNK>\", 0)) for gene in gene_names]  # Map gene names to indices\n",
    "        tokenized_data.append(tokens)\n",
    "    \n",
    "    return torch.tensor(tokenized_data)\n",
    "\n",
    "# Convert dataset into tokenized format\n",
    "input_tokens = tokenize_gene_expression(adata, vocab)\n",
    "\n",
    "print(\"✅ Data tokenized successfully!\")\n",
    "print(\"Sample Tokenized Data:\", input_tokens[:5])  # Show first 5 cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e123bd2d-1bf8-41b3-bee8-286725d17cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 352734 cells in batches of 1000...\n",
      "Processed 1000/352734 cells\n",
      "Processed 2000/352734 cells\n",
      "Processed 3000/352734 cells\n",
      "Processed 4000/352734 cells\n",
      "Processed 5000/352734 cells\n",
      "Processed 6000/352734 cells\n",
      "Processed 7000/352734 cells\n",
      "Processed 8000/352734 cells\n",
      "Processed 9000/352734 cells\n",
      "Processed 10000/352734 cells\n",
      "Processed 11000/352734 cells\n",
      "Processed 12000/352734 cells\n",
      "Processed 13000/352734 cells\n",
      "Processed 14000/352734 cells\n",
      "Processed 15000/352734 cells\n",
      "Processed 16000/352734 cells\n",
      "Processed 17000/352734 cells\n",
      "Processed 18000/352734 cells\n",
      "Processed 19000/352734 cells\n",
      "Processed 20000/352734 cells\n",
      "Processed 21000/352734 cells\n",
      "Processed 22000/352734 cells\n",
      "Processed 23000/352734 cells\n",
      "Processed 24000/352734 cells\n",
      "Processed 25000/352734 cells\n",
      "Processed 26000/352734 cells\n",
      "Processed 27000/352734 cells\n",
      "Processed 28000/352734 cells\n",
      "Processed 29000/352734 cells\n",
      "Processed 30000/352734 cells\n",
      "Processed 31000/352734 cells\n",
      "Processed 32000/352734 cells\n",
      "Processed 33000/352734 cells\n",
      "Processed 34000/352734 cells\n",
      "Processed 35000/352734 cells\n",
      "Processed 36000/352734 cells\n",
      "Processed 37000/352734 cells\n",
      "Processed 38000/352734 cells\n",
      "Processed 39000/352734 cells\n",
      "Processed 40000/352734 cells\n",
      "Processed 41000/352734 cells\n",
      "Processed 42000/352734 cells\n",
      "Processed 43000/352734 cells\n",
      "Processed 44000/352734 cells\n",
      "Processed 45000/352734 cells\n",
      "Processed 46000/352734 cells\n",
      "Processed 47000/352734 cells\n",
      "Processed 48000/352734 cells\n",
      "Processed 49000/352734 cells\n",
      "Processed 50000/352734 cells\n",
      "Processed 51000/352734 cells\n",
      "Processed 52000/352734 cells\n",
      "Processed 53000/352734 cells\n",
      "Processed 54000/352734 cells\n",
      "Processed 55000/352734 cells\n",
      "Processed 56000/352734 cells\n",
      "Processed 57000/352734 cells\n",
      "Processed 58000/352734 cells\n",
      "Processed 59000/352734 cells\n",
      "Processed 60000/352734 cells\n",
      "Processed 61000/352734 cells\n",
      "Processed 62000/352734 cells\n",
      "Processed 63000/352734 cells\n",
      "Processed 64000/352734 cells\n",
      "Processed 65000/352734 cells\n",
      "Processed 66000/352734 cells\n",
      "Processed 67000/352734 cells\n",
      "Processed 68000/352734 cells\n",
      "Processed 69000/352734 cells\n",
      "Processed 70000/352734 cells\n",
      "Processed 71000/352734 cells\n",
      "Processed 72000/352734 cells\n",
      "Processed 73000/352734 cells\n",
      "Processed 74000/352734 cells\n",
      "Processed 75000/352734 cells\n",
      "Processed 76000/352734 cells\n",
      "Processed 77000/352734 cells\n",
      "Processed 78000/352734 cells\n",
      "Processed 79000/352734 cells\n",
      "Processed 80000/352734 cells\n",
      "Processed 81000/352734 cells\n",
      "Processed 82000/352734 cells\n",
      "Processed 83000/352734 cells\n",
      "Processed 84000/352734 cells\n",
      "Processed 85000/352734 cells\n",
      "Processed 86000/352734 cells\n",
      "Processed 87000/352734 cells\n",
      "Processed 88000/352734 cells\n",
      "Processed 89000/352734 cells\n",
      "Processed 90000/352734 cells\n",
      "Processed 91000/352734 cells\n",
      "Processed 92000/352734 cells\n",
      "Processed 93000/352734 cells\n",
      "Processed 94000/352734 cells\n",
      "Processed 95000/352734 cells\n",
      "Processed 96000/352734 cells\n",
      "Processed 97000/352734 cells\n",
      "Processed 98000/352734 cells\n",
      "Processed 99000/352734 cells\n",
      "Processed 100000/352734 cells\n",
      "Processed 101000/352734 cells\n",
      "Processed 102000/352734 cells\n",
      "Processed 103000/352734 cells\n",
      "Processed 104000/352734 cells\n",
      "Processed 105000/352734 cells\n",
      "Processed 106000/352734 cells\n",
      "Processed 107000/352734 cells\n",
      "Processed 108000/352734 cells\n",
      "Processed 109000/352734 cells\n",
      "Processed 110000/352734 cells\n",
      "Processed 111000/352734 cells\n",
      "Processed 112000/352734 cells\n",
      "Processed 113000/352734 cells\n",
      "Processed 114000/352734 cells\n",
      "Processed 115000/352734 cells\n",
      "Processed 116000/352734 cells\n",
      "Processed 117000/352734 cells\n",
      "Processed 118000/352734 cells\n",
      "Processed 119000/352734 cells\n",
      "Processed 120000/352734 cells\n",
      "Processed 121000/352734 cells\n",
      "Processed 122000/352734 cells\n",
      "Processed 123000/352734 cells\n",
      "Processed 124000/352734 cells\n",
      "Processed 125000/352734 cells\n",
      "Processed 126000/352734 cells\n",
      "Processed 127000/352734 cells\n",
      "Processed 128000/352734 cells\n",
      "Processed 129000/352734 cells\n",
      "Processed 130000/352734 cells\n",
      "Processed 131000/352734 cells\n",
      "Processed 132000/352734 cells\n",
      "Processed 133000/352734 cells\n",
      "Processed 134000/352734 cells\n",
      "Processed 135000/352734 cells\n",
      "Processed 136000/352734 cells\n",
      "Processed 137000/352734 cells\n",
      "Processed 138000/352734 cells\n",
      "Processed 139000/352734 cells\n",
      "Processed 140000/352734 cells\n",
      "Processed 141000/352734 cells\n",
      "Processed 142000/352734 cells\n",
      "Processed 143000/352734 cells\n",
      "Processed 144000/352734 cells\n",
      "Processed 145000/352734 cells\n",
      "Processed 146000/352734 cells\n",
      "Processed 147000/352734 cells\n",
      "Processed 148000/352734 cells\n",
      "Processed 149000/352734 cells\n",
      "Processed 150000/352734 cells\n",
      "Processed 151000/352734 cells\n",
      "Processed 152000/352734 cells\n",
      "Processed 153000/352734 cells\n",
      "Processed 154000/352734 cells\n",
      "Processed 155000/352734 cells\n",
      "Processed 156000/352734 cells\n",
      "Processed 157000/352734 cells\n",
      "Processed 158000/352734 cells\n",
      "Processed 159000/352734 cells\n",
      "Processed 160000/352734 cells\n",
      "Processed 161000/352734 cells\n",
      "Processed 162000/352734 cells\n",
      "Processed 163000/352734 cells\n",
      "Processed 164000/352734 cells\n",
      "Processed 165000/352734 cells\n",
      "Processed 166000/352734 cells\n",
      "Processed 167000/352734 cells\n",
      "Processed 168000/352734 cells\n",
      "Processed 169000/352734 cells\n",
      "Processed 170000/352734 cells\n",
      "Processed 171000/352734 cells\n",
      "Processed 172000/352734 cells\n",
      "Processed 173000/352734 cells\n",
      "Processed 174000/352734 cells\n",
      "Processed 175000/352734 cells\n",
      "Processed 176000/352734 cells\n",
      "Processed 177000/352734 cells\n",
      "Processed 178000/352734 cells\n",
      "Processed 179000/352734 cells\n",
      "Processed 180000/352734 cells\n",
      "Processed 181000/352734 cells\n",
      "Processed 182000/352734 cells\n",
      "Processed 183000/352734 cells\n",
      "Processed 184000/352734 cells\n",
      "Processed 185000/352734 cells\n",
      "Processed 186000/352734 cells\n",
      "Processed 187000/352734 cells\n",
      "Processed 188000/352734 cells\n",
      "Processed 189000/352734 cells\n",
      "Processed 190000/352734 cells\n",
      "Processed 191000/352734 cells\n",
      "Processed 192000/352734 cells\n",
      "Processed 193000/352734 cells\n",
      "Processed 194000/352734 cells\n",
      "Processed 195000/352734 cells\n",
      "Processed 196000/352734 cells\n",
      "Processed 197000/352734 cells\n",
      "Processed 198000/352734 cells\n",
      "Processed 199000/352734 cells\n",
      "Processed 200000/352734 cells\n",
      "Processed 201000/352734 cells\n",
      "Processed 202000/352734 cells\n",
      "Processed 203000/352734 cells\n",
      "Processed 204000/352734 cells\n",
      "Processed 205000/352734 cells\n",
      "Processed 206000/352734 cells\n",
      "Processed 207000/352734 cells\n",
      "Processed 208000/352734 cells\n",
      "Processed 209000/352734 cells\n",
      "Processed 210000/352734 cells\n",
      "Processed 211000/352734 cells\n",
      "Processed 212000/352734 cells\n",
      "Processed 213000/352734 cells\n",
      "Processed 214000/352734 cells\n",
      "Processed 215000/352734 cells\n",
      "Processed 216000/352734 cells\n",
      "Processed 217000/352734 cells\n",
      "Processed 218000/352734 cells\n",
      "Processed 219000/352734 cells\n",
      "Processed 220000/352734 cells\n",
      "Processed 221000/352734 cells\n",
      "Processed 222000/352734 cells\n",
      "Processed 223000/352734 cells\n",
      "Processed 224000/352734 cells\n",
      "Processed 225000/352734 cells\n",
      "Processed 226000/352734 cells\n",
      "Processed 227000/352734 cells\n",
      "Processed 228000/352734 cells\n",
      "Processed 229000/352734 cells\n",
      "Processed 230000/352734 cells\n",
      "Processed 231000/352734 cells\n",
      "Processed 232000/352734 cells\n",
      "Processed 233000/352734 cells\n",
      "Processed 234000/352734 cells\n",
      "Processed 235000/352734 cells\n",
      "Processed 236000/352734 cells\n",
      "Processed 237000/352734 cells\n",
      "Processed 238000/352734 cells\n",
      "Processed 239000/352734 cells\n",
      "Processed 240000/352734 cells\n",
      "Processed 241000/352734 cells\n",
      "Processed 242000/352734 cells\n",
      "Processed 243000/352734 cells\n",
      "Processed 244000/352734 cells\n",
      "Processed 245000/352734 cells\n",
      "Processed 246000/352734 cells\n",
      "Processed 247000/352734 cells\n",
      "Processed 248000/352734 cells\n",
      "Processed 249000/352734 cells\n",
      "Processed 250000/352734 cells\n",
      "Processed 251000/352734 cells\n",
      "Processed 252000/352734 cells\n",
      "Processed 253000/352734 cells\n",
      "Processed 254000/352734 cells\n",
      "Processed 255000/352734 cells\n",
      "Processed 256000/352734 cells\n",
      "Processed 257000/352734 cells\n",
      "Processed 258000/352734 cells\n",
      "Processed 259000/352734 cells\n",
      "Processed 260000/352734 cells\n",
      "Processed 261000/352734 cells\n",
      "Processed 262000/352734 cells\n",
      "Processed 263000/352734 cells\n",
      "Processed 264000/352734 cells\n",
      "Processed 265000/352734 cells\n",
      "Processed 266000/352734 cells\n",
      "Processed 267000/352734 cells\n",
      "Processed 268000/352734 cells\n",
      "Processed 269000/352734 cells\n",
      "Processed 270000/352734 cells\n",
      "Processed 271000/352734 cells\n",
      "Processed 272000/352734 cells\n",
      "Processed 273000/352734 cells\n",
      "Processed 274000/352734 cells\n",
      "Processed 275000/352734 cells\n",
      "Processed 276000/352734 cells\n",
      "Processed 277000/352734 cells\n",
      "Processed 278000/352734 cells\n",
      "Processed 279000/352734 cells\n",
      "Processed 280000/352734 cells\n",
      "Processed 281000/352734 cells\n",
      "Processed 282000/352734 cells\n",
      "Processed 283000/352734 cells\n",
      "Processed 284000/352734 cells\n",
      "Processed 285000/352734 cells\n",
      "Processed 286000/352734 cells\n",
      "Processed 287000/352734 cells\n",
      "Processed 288000/352734 cells\n",
      "Processed 289000/352734 cells\n",
      "Processed 290000/352734 cells\n",
      "Processed 291000/352734 cells\n",
      "Processed 292000/352734 cells\n",
      "Processed 293000/352734 cells\n",
      "Processed 294000/352734 cells\n",
      "Processed 295000/352734 cells\n",
      "Processed 296000/352734 cells\n",
      "Processed 297000/352734 cells\n",
      "Processed 298000/352734 cells\n",
      "Processed 299000/352734 cells\n",
      "Processed 300000/352734 cells\n",
      "Processed 301000/352734 cells\n",
      "Processed 302000/352734 cells\n",
      "Processed 303000/352734 cells\n",
      "Processed 304000/352734 cells\n",
      "Processed 305000/352734 cells\n",
      "Processed 306000/352734 cells\n",
      "Processed 307000/352734 cells\n",
      "Processed 308000/352734 cells\n",
      "Processed 309000/352734 cells\n",
      "Processed 310000/352734 cells\n",
      "Processed 311000/352734 cells\n",
      "Processed 312000/352734 cells\n",
      "Processed 313000/352734 cells\n",
      "Processed 314000/352734 cells\n",
      "Processed 315000/352734 cells\n",
      "Processed 316000/352734 cells\n",
      "Processed 317000/352734 cells\n",
      "Processed 318000/352734 cells\n",
      "Processed 319000/352734 cells\n",
      "Processed 320000/352734 cells\n",
      "Processed 321000/352734 cells\n",
      "Processed 322000/352734 cells\n",
      "Processed 323000/352734 cells\n",
      "Processed 324000/352734 cells\n",
      "Processed 325000/352734 cells\n",
      "Processed 326000/352734 cells\n",
      "Processed 327000/352734 cells\n",
      "Processed 328000/352734 cells\n",
      "Processed 329000/352734 cells\n",
      "Processed 330000/352734 cells\n",
      "Processed 331000/352734 cells\n",
      "Processed 332000/352734 cells\n",
      "Processed 333000/352734 cells\n",
      "Processed 334000/352734 cells\n",
      "Processed 335000/352734 cells\n",
      "Processed 336000/352734 cells\n",
      "Processed 337000/352734 cells\n",
      "Processed 338000/352734 cells\n",
      "Processed 339000/352734 cells\n",
      "Processed 340000/352734 cells\n",
      "Processed 341000/352734 cells\n",
      "Processed 342000/352734 cells\n",
      "Processed 343000/352734 cells\n",
      "Processed 344000/352734 cells\n",
      "Processed 345000/352734 cells\n",
      "Processed 346000/352734 cells\n",
      "Processed 347000/352734 cells\n",
      "Processed 348000/352734 cells\n",
      "Processed 349000/352734 cells\n",
      "Processed 350000/352734 cells\n",
      "Processed 351000/352734 cells\n",
      "Processed 352000/352734 cells\n",
      "Processed 352734/352734 cells\n",
      "Tokenized data saved to: tokenized_data.h5\n",
      "Tokenization completed and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import h5py\n",
    "\n",
    "def tokenize_gene_expression(adata, vocab, batch_size=1000, output_file=\"tokenized_data.h5\"):\n",
    "    \"\"\"\n",
    "    Convert gene expression data to tokenized form using scGPT vocabulary in batches and save to disk.\n",
    "    This prevents high memory usage and kernel crashes.\n",
    "    \"\"\"\n",
    "    gene_names = adata.var[\"feature_name\"].tolist()  # Extract gene names\n",
    "    stoi = vocab.get_stoi()  # Get gene-to-index mapping\n",
    "    num_cells = adata.shape[0]  # Number of cells\n",
    "    num_genes = len(gene_names)\n",
    "\n",
    "    print(f\"Tokenizing {num_cells} cells in batches of {batch_size}...\")\n",
    "\n",
    "    # **Create an HDF5 file to store tokenized data**\n",
    "    with h5py.File(output_file, \"w\") as hf:\n",
    "        tokenized_dataset = hf.create_dataset(\"tokenized_data\", shape=(num_cells, num_genes), dtype=\"int32\")\n",
    "\n",
    "        for i in range(0, num_cells, batch_size):\n",
    "            batch_end = min(i + batch_size, num_cells)\n",
    "            batch = adata.X[i:batch_end].todense() if hasattr(adata.X, \"todense\") else adata.X[i:batch_end]\n",
    "\n",
    "            tokens = np.array([[stoi.get(gene, stoi.get(\"<UNK>\", 0)) for gene in gene_names] for _ in range(batch.shape[0])], dtype=\"int32\")\n",
    "            tokenized_dataset[i:batch_end, :] = tokens  # Write batch to disk\n",
    "\n",
    "            print(f\"Processed {batch_end}/{num_cells} cells\")\n",
    "\n",
    "    print(\"Tokenized data saved to:\", output_file)\n",
    "\n",
    "# **Run Tokenization & Save to Disk**\n",
    "tokenize_gene_expression(adata, vocab)\n",
    "\n",
    "print(\"Tokenization completed and saved to disk.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63591dd-e20c-4151-bcc8-7f84136e8876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['tokenized_data']\n",
      "Tokenized Data Shape: (352734, 45453)\n",
      "Sample Tokenized Values (First 5 Cells):\n",
      " [[34494 33849  7983 ... 60280 60700 60700]\n",
      " [34494 33849  7983 ... 60280 60700 60700]\n",
      " [34494 33849  7983 ... 60280 60700 60700]\n",
      " [34494 33849  7983 ... 60280 60700 60700]\n",
      " [34494 33849  7983 ... 60280 60700 60700]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Define the path to the saved tokenized file\n",
    "tokenized_file = \"tokenized_data.h5\"\n",
    "\n",
    "# Open HDF5 file and check dataset details\n",
    "with h5py.File(tokenized_file, \"r\") as hf:\n",
    "    print(\"Available datasets:\", list(hf.keys()))  # List datasets in file\n",
    "    tokenized_data = hf[\"tokenized_data\"]  # Load dataset\n",
    "    print(\"Tokenized Data Shape:\", tokenized_data.shape)  # Check dimensions (cells × genes)\n",
    "    print(\"Sample Tokenized Values (First 5 Cells):\\n\", tokenized_data[:5])  # View first 5 tokenized rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de2984d-a79a-42c4-9010-83bac7a36ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ready to load batches for inference.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "\n",
    "# Define the path to the tokenized dataset\n",
    "tokenized_file = \"tokenized_data.h5\"\n",
    "\n",
    "# Open HDF5 file for batch processing\n",
    "def load_batches(batch_size=1000):\n",
    "    with h5py.File(tokenized_file, \"r\") as hf:\n",
    "        tokenized_data = hf[\"tokenized_data\"]\n",
    "        num_cells = tokenized_data.shape[0]\n",
    "        \n",
    "        for i in range(0, num_cells, batch_size):\n",
    "            batch = torch.tensor(tokenized_data[i: i + batch_size], dtype=torch.long)\n",
    "            yield batch.to(device)\n",
    "\n",
    "print(\" Ready to load batches for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bbedae-267a-4a92-a059-5dcc76300a14",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'values' and 'src_key_padding_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m load_batches(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Convert to list\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mextend(batch_predictions)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'values' and 'src_key_padding_mask'"
     ]
    }
   ],
   "source": [
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store predictions\n",
    "predictions = []\n",
    "\n",
    "# Run inference in batches\n",
    "with torch.no_grad():\n",
    "    for batch in load_batches(batch_size=1000):\n",
    "        outputs = model(batch)\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()  # Convert to list\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "print(\"Inference completed. Total predictions:\", len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7386e5d8-e61e-4f07-8635-63e52858124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to load batches for inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_batches(batch_size=1000):\n",
    "    with h5py.File(tokenized_file, \"r\") as hf:\n",
    "        tokenized_data = hf[\"tokenized_data\"]\n",
    "        num_cells = tokenized_data.shape[0]\n",
    "        \n",
    "        for i in range(0, num_cells, batch_size):\n",
    "            batch_tokens = torch.tensor(tokenized_data[i: i + batch_size], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Extract gene expression values for the batch\n",
    "            batch_values = torch.tensor(adata.X[i: i + batch_size].todense() if hasattr(adata.X, \"todense\") else adata.X[i: i + batch_size], dtype=torch.float).to(device)\n",
    "\n",
    "            # Create a padding mask (1 for padding, 0 for real tokens)\n",
    "            pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "            if pad_index is None:\n",
    "                raise ValueError(\"The <PAD> token is missing from the vocabulary.\")\n",
    "\n",
    "            src_key_padding_mask = (batch_tokens == pad_index)\n",
    "\n",
    "            yield batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "print(\"Ready to load batches for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeda1617-3905-4d14-ae70-507068ce05c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.70 GiB. GPU 0 has a total capacty of 23.61 GiB of which 19.71 GiB is free. Process 10102 has 988.00 MiB memory in use. Including non-PyTorch memory, this process has 1.98 GiB memory in use. Of the allocated memory 1.46 GiB is allocated by PyTorch, and 87.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_tokens, batch_values, src_key_padding_mask \u001b[38;5;129;01min\u001b[39;00m load_batches(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Convert to list\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mextend(batch_predictions)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[1;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:176\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode\u001b[39m(\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    169\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     batch_labels: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# (batch,)\u001b[39;00m\n\u001b[1;32m    173\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_batch_labels(batch_labels)\n\u001b[0;32m--> 176\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, seq_len, embsize)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_gene_token_embs \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_encoder(values)  \u001b[38;5;66;03m# (batch, seq_len, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:737\u001b[0m, in \u001b[0;36mGeneEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 737\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, seq_len, embsize)\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_norm(x)\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.70 GiB. GPU 0 has a total capacty of 23.61 GiB of which 19.71 GiB is free. Process 10102 has 988.00 MiB memory in use. Including non-PyTorch memory, this process has 1.98 GiB memory in use. Of the allocated memory 1.46 GiB is allocated by PyTorch, and 87.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store predictions\n",
    "predictions = []\n",
    "\n",
    "# Run inference in batches\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in load_batches(batch_size=1000):\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()  # Convert to list\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "print(\" Inference completed. Total predictions:\", len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87bf2cd6-7d84-450f-a47a-40f7fc84f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to load batches for inference with batch size: 25\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "\n",
    "# Set batch size dynamically (start with 500, reduce if needed)\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "def load_batches(batch_size=BATCH_SIZE):\n",
    "    with h5py.File(tokenized_file, \"r\") as hf:\n",
    "        tokenized_data = hf[\"tokenized_data\"]\n",
    "        num_cells = tokenized_data.shape[0]\n",
    "        \n",
    "        for i in range(0, num_cells, batch_size):\n",
    "            batch_tokens = torch.tensor(tokenized_data[i: i + batch_size], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Extract gene expression values and convert to half precision (float16)\n",
    "            batch_values = torch.tensor(\n",
    "                adata.X[i: i + batch_size].todense() if hasattr(adata.X, \"todense\") else adata.X[i: i + batch_size], \n",
    "                dtype=torch.float16\n",
    "            ).to(device)\n",
    "\n",
    "            # Create a padding mask (1 for padding, 0 for real tokens)\n",
    "            pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "            if pad_index is None:\n",
    "                raise ValueError(\"The <PAD> token is missing from the vocabulary.\")\n",
    "\n",
    "            src_key_padding_mask = (batch_tokens == pad_index)\n",
    "\n",
    "            yield batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "print(\"Ready to load batches for inference with batch size:\", BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8ca379-8234-47c0-8a3e-1076cdf1cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optimized batch loading enabled with batch size: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Set small batch size for extreme memory optimization\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Create memory-mapped storage for predictions (avoids RAM usage)\n",
    "num_cells = 352734  # Update based on dataset size\n",
    "predictions_file = \"scGPT_predictions.npy\"\n",
    "predictions_memmap = np.memmap(predictions_file, dtype=\"int32\", mode=\"w+\", shape=(num_cells,))\n",
    "\n",
    "def load_batches(batch_size=BATCH_SIZE):\n",
    "    with h5py.File(\"tokenized_data.h5\", \"r\") as hf:\n",
    "        tokenized_data = hf[\"tokenized_data\"]\n",
    "        num_cells = tokenized_data.shape[0]\n",
    "\n",
    "        for i in range(0, num_cells, batch_size):\n",
    "            batch_tokens = torch.tensor(tokenized_data[i: i + batch_size], dtype=torch.long)\n",
    "\n",
    "            # Load only a portion of gene expression values from disk\n",
    "            batch_values = torch.tensor(\n",
    "                adata.X[i: i + batch_size].todense() if hasattr(adata.X, \"todense\") else adata.X[i: i + batch_size],\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            # Create padding mask\n",
    "            pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "            if pad_index is None:\n",
    "                raise ValueError(\"The <PAD> token is missing from the vocabulary.\")\n",
    "\n",
    "            src_key_padding_mask = (batch_tokens == pad_index)\n",
    "\n",
    "            yield batch_tokens, batch_values, src_key_padding_mask, i\n",
    "\n",
    "print(\" Optimized batch loading enabled with batch size:\", BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "170a787f-5f70-4ccc-9e79-77e9212b56bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 661112066880 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m src_key_padding_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Convert predictions to NumPy and save directly to disk\u001b[39;00m\n\u001b[1;32m     19\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[1;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:194\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(total_embs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:678\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[1;32m    677\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mmerge_masks(src_mask, src_key_padding_mask, src)\n\u001b[0;32m--> 678\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer_encoder_layer_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_relu_or_gelu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 661112066880 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# Force model to run on CPU (avoids GPU memory issues)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run inference batch-by-batch and write predictions directly to disk\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask, batch_index in load_batches():\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Convert predictions to NumPy and save directly to disk\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        predictions_memmap[batch_index: batch_index + BATCH_SIZE] = batch_predictions\n",
    "\n",
    "        # Free up memory\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Inference completed. Predictions saved directly to disk.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8039ab71-478f-40e5-9b1e-2f9515425d3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.17 GiB. GPU 0 has a total capacty of 23.61 GiB of which 232.94 MiB is free. Process 10102 has 988.00 MiB memory in use. Including non-PyTorch memory, this process has 21.51 GiB memory in use. Of the allocated memory 18.98 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m src_key_padding_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert predictions to list\u001b[39;00m\n\u001b[1;32m     21\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[1;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:179\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    176\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src)  \u001b[38;5;66;03m# (batch, seq_len, embsize)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_gene_token_embs \u001b[38;5;241m=\u001b[39m src\n\u001b[0;32m--> 179\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, seq_len, embsize)\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_emb_style \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    181\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:789\u001b[0m, in \u001b[0;36mContinuousValueEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# clip x to [-inf, max_value]\u001b[39;00m\n\u001b[1;32m    788\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(x, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_value)\n\u001b[0;32m--> 789\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x)\n\u001b[1;32m    791\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/functional.py:1471\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.17 GiB. GPU 0 has a total capacty of 23.61 GiB of which 232.94 MiB is free. Process 10102 has 988.00 MiB memory in use. Including non-PyTorch memory, this process has 21.51 GiB memory in use. Of the allocated memory 18.98 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store predictions\n",
    "predictions = []\n",
    "\n",
    "# Run inference in batches\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in load_batches():\n",
    "        batch_tokens = batch_tokens.to(torch.int64)  # Ensure correct dtype\n",
    "        batch_values = batch_values.to(torch.float32)  # Convert to float32\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Convert predictions to list\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Free up memory after each batch\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Inference completed. Total predictions:\", len(predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b4ee82-7007-4c44-9abc-e72fdb993319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced gene dimension: 10000 genes\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Keep only the top 10,000 most variable genes\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=10000)\n",
    "adata = adata[:, adata.var[\"highly_variable\"]]\n",
    "\n",
    "print(f\"Reduced gene dimension: {adata.shape[1]} genes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d50a611a-1d29-4d09-8f2d-8e05f80be9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, tokenized_file, adata):\n",
    "        self.h5_file = tokenized_file\n",
    "        self.num_cells = adata.shape[0]\n",
    "        self.adata = adata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "        \n",
    "        batch_values = torch.tensor(\n",
    "            self.adata.X[idx].todense() if hasattr(self.adata.X, \"todense\") else self.adata.X[idx],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "        src_key_padding_mask = (batch_tokens == pad_index)\n",
    "        \n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "print(\"Dataset class defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ece2f6-9c5a-458f-9082-ec60c7136903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DataLoader initialized with batch size: 10\n"
     ]
    }
   ],
   "source": [
    "# Define dataset and dataloader\n",
    "tokenized_file = \"tokenized_data.h5\"\n",
    "dataset = scGPTDataset(tokenized_file, adata)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(f\" DataLoader initialized with batch size: {10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea44762-981a-4711-bc3d-f15fece9b7ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (45453) must match the size of tensor b (10000) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m src_key_padding_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Run inference on CPU\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Convert predictions to NumPy and save in list\u001b[39;00m\n\u001b[1;32m     24\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[1;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:184\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    182\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m*\u001b[39m values\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdsbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     batch_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (45453) must match the size of tensor b (10000) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Force model to run on CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Store predictions\n",
    "predictions = []\n",
    "\n",
    "# Run inference batch-by-batch with DataLoader\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference on CPU\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Convert predictions to NumPy and save in list\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Free memory after each batch\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        gc.collect()\n",
    "\n",
    "print(\" Inference completed. Predictions stored successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276fd95c-c433-401d-9313-c30312735532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6705 common genes between adata and vocab.\n"
     ]
    }
   ],
   "source": [
    "# Get the genes available in adata after filtering\n",
    "adata_genes = set(adata.var[\"feature_name\"].tolist())\n",
    "\n",
    "# Get the genes available in the vocabulary\n",
    "vocab_genes = set(vocab.get_itos())  # Index-to-string mapping\n",
    "\n",
    "# Find the common genes between both\n",
    "common_genes = list(adata_genes & vocab_genes)\n",
    "\n",
    "print(f\"Found {len(common_genes)} common genes between adata and vocab.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7f068-ee5c-4d5e-968e-13fa7496d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenized data\n",
    "with h5py.File(\"tokenized_data.h5\", \"r\") as hf:\n",
    "    original_tokens = hf[\"tokenized_data\"][:]  # Load all tokens into memory\n",
    "\n",
    "# Get indices of common genes in vocab\n",
    "stoi = vocab.get_stoi()  # String-to-index mapping\n",
    "common_gene_indices = [stoi[gene] for gene in common_genes if gene in stoi]\n",
    "\n",
    "# Select only these indices\n",
    "filtered_tokens = original_tokens[:, common_gene_indices]  # Select matching columns\n",
    "\n",
    "# Save new filtered dataset\n",
    "with h5py.File(\"filtered_tokenized_data.h5\", \"w\") as hf:\n",
    "    hf.create_dataset(\"tokenized_data\", data=filtered_tokens)\n",
    "\n",
    "print(f\"Filtered tokenized data saved with {filtered_tokens.shape[1]} genes (6,705).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320c3bc9-2333-47a0-b311-ff3f2d86c3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Memory cleared. Ready for inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear CUDA and CPU memory before inference\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\" Memory cleared. Ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773ce635-2290-4630-8ce6-bae7fb1cdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5  # Reduce to avoid memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3907ff5-4a43-4e5f-a2ab-1ed7c7be0204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DataLoader initialized with batch size: 5 (Lazy loading enabled).\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, tokenized_file, adata):\n",
    "        self.h5_file = tokenized_file\n",
    "        self.num_cells = adata.shape[0]\n",
    "        self.adata = adata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "        \n",
    "        batch_values = torch.tensor(\n",
    "            self.adata.X[idx].todense() if hasattr(self.adata.X, \"todense\") else self.adata.X[idx],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "        src_key_padding_mask = (batch_tokens == pad_index)\n",
    "        \n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "# **Use the new filtered dataset**\n",
    "dataset = scGPTDataset(\"filtered_tokenized_data.h5\", adata)\n",
    "\n",
    "# **Initialize DataLoader with extreme memory optimizations**\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\" DataLoader initialized with batch size: {BATCH_SIZE} (Lazy loading enabled).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "782d3a83-df38-49ff-88c8-a94b4d548707",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'filtered_tokenized_data.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run inference batch-by-batch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_tokens, batch_values, src_key_padding_mask \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     12\u001b[0m         batch_tokens \u001b[38;5;241m=\u001b[39m batch_tokens\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m         batch_values \u001b[38;5;241m=\u001b[39m batch_values\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mscGPTDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh5_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m hf:\n\u001b[1;32m     15\u001b[0m         batch_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(hf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_data\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     17\u001b[0m     batch_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata\u001b[38;5;241m.\u001b[39mX[idx]\u001b[38;5;241m.\u001b[39mtodense() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata\u001b[38;5;241m.\u001b[39mX, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtodense\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata\u001b[38;5;241m.\u001b[39mX[idx],\n\u001b[1;32m     19\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m     20\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'filtered_tokenized_data.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Move model to CPU (Prevents GPU out-of-memory errors)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Store predictions\n",
    "predictions = []\n",
    "\n",
    "# Run inference batch-by-batch\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Convert to predictions\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # **Free memory after each batch**\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Inference completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c0bdeb-64a1-4578-8899-22a19373af5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " File not found. Regenerating it now...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"filtered_tokenized_data.h5\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\" File exists. Ready for loading.\")\n",
    "else:\n",
    "    print(\" File not found. Regenerating it now...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d18f33-688c-40d8-a7e8-1f18bf583eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Load original tokenized data\n",
    "with h5py.File(\"tokenized_data.h5\", \"r\") as hf:\n",
    "    original_tokens = hf[\"tokenized_data\"][:]  # Load all tokens\n",
    "\n",
    "# Get indices of common genes in vocab\n",
    "stoi = vocab.get_stoi()  # String-to-index mapping\n",
    "common_gene_indices = [stoi[gene] for gene in common_genes if gene in stoi]\n",
    "\n",
    "# Select only these indices\n",
    "filtered_tokens = original_tokens[:, common_gene_indices]  # Keep only 6705 genes\n",
    "\n",
    "# Save the filtered dataset\n",
    "with h5py.File(\"filtered_tokenized_data.h5\", \"w\") as hf:\n",
    "    hf.create_dataset(\"tokenized_data\", data=filtered_tokens)\n",
    "\n",
    "print(f\" Filtered tokenized data saved successfully with {filtered_tokens.shape[1]} genes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e205fc-b927-4f2d-b555-1eede6069413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Original dataset: 352734 cells, 45453 genes\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 55057 is out of bounds for axis 1 with size 45453",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_data.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hf_in:\n\u001b[1;32m     25\u001b[0m     chunk_tokens \u001b[38;5;241m=\u001b[39m hf_in[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_data\u001b[39m\u001b[38;5;124m\"\u001b[39m][start:end, :]  \u001b[38;5;66;03m# Load only small part\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     filtered_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_gene_indices\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Keep only required genes\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     filtered_dataset[start:end, :] \u001b[38;5;241m=\u001b[39m filtered_chunk  \u001b[38;5;66;03m# Save chunk\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 55057 is out of bounds for axis 1 with size 45453"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the original HDF5 file\n",
    "with h5py.File(\"tokenized_data.h5\", \"r\") as hf:\n",
    "    total_cells, total_genes = hf[\"tokenized_data\"].shape\n",
    "    print(f\"🔹 Original dataset: {total_cells} cells, {total_genes} genes\")\n",
    "\n",
    "# Get indices of common genes in vocab\n",
    "stoi = vocab.get_stoi()  # String-to-index mapping\n",
    "common_gene_indices = [stoi[gene] for gene in common_genes if gene in stoi]\n",
    "\n",
    "# **Create the new filtered HDF5 file**\n",
    "with h5py.File(\"filtered_tokenized_data.h5\", \"w\") as hf_out:\n",
    "    filtered_dataset = hf_out.create_dataset(\n",
    "        \"tokenized_data\", shape=(total_cells, len(common_gene_indices)), dtype=\"int32\"\n",
    "    )\n",
    "\n",
    "    # **Process in chunks of 10,000 cells**\n",
    "    CHUNK_SIZE = 10000\n",
    "    for start in range(0, total_cells, CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, total_cells)\n",
    "\n",
    "        with h5py.File(\"tokenized_data.h5\", \"r\") as hf_in:\n",
    "            chunk_tokens = hf_in[\"tokenized_data\"][start:end, :]  # Load only small part\n",
    "            filtered_chunk = chunk_tokens[:, common_gene_indices]  # Keep only required genes\n",
    "            filtered_dataset[start:end, :] = filtered_chunk  # Save chunk\n",
    "\n",
    "        print(f\" Processed {end}/{total_cells} cells\")\n",
    "\n",
    "print(f\" Filtered dataset saved with {filtered_dataset.shape[1]} genes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07e7d083-bf8e-4fca-bbc7-22d8599a34ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unable to synchronously open object (object 'gene_names' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get original gene names from tokenized_data.h5\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_data.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hf:\n\u001b[0;32m----> 3\u001b[0m     original_gene_names \u001b[38;5;241m=\u001b[39m \u001b[43mhf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgene_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[:]  \u001b[38;5;66;03m# Load gene names\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     original_gene_names \u001b[38;5;241m=\u001b[39m [g\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m original_gene_names]  \u001b[38;5;66;03m# Convert bytes to strings\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert to dictionary for fast lookup\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:257\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to synchronously open object (object 'gene_names' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "# Get original gene names from tokenized_data.h5\n",
    "with h5py.File(\"tokenized_data.h5\", \"r\") as hf:\n",
    "    original_gene_names = hf[\"gene_names\"][:]  # Load gene names\n",
    "    original_gene_names = [g.decode(\"utf-8\") for g in original_gene_names]  # Convert bytes to strings\n",
    "\n",
    "# Convert to dictionary for fast lookup\n",
    "gene_to_index = {gene: i for i, gene in enumerate(original_gene_names)}\n",
    "\n",
    "# Ensure only valid genes are included\n",
    "valid_common_genes = [gene for gene in common_genes if gene in gene_to_index]\n",
    "common_gene_indices = [gene_to_index[gene] for gene in valid_common_genes]\n",
    "\n",
    "print(f\" Found {len(valid_common_genes)} valid genes in tokenized_data.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69f00989-a012-4344-9a56-f77e2a5cddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 6705 valid genes in adata\n"
     ]
    }
   ],
   "source": [
    "# Get original gene names from adata (instead of HDF5)\n",
    "original_gene_names = adata.var[\"feature_name\"].tolist()\n",
    "gene_to_index = {gene: i for i, gene in enumerate(original_gene_names)}\n",
    "\n",
    "# Ensure only valid genes are included\n",
    "valid_common_genes = [gene for gene in common_genes if gene in gene_to_index]\n",
    "common_gene_indices = [gene_to_index[gene] for gene in valid_common_genes]\n",
    "\n",
    "print(f\" Found {len(valid_common_genes)} valid genes in adata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9a625b3-a2dc-4579-aa56-4e11bec129a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Original dataset: 352734 cells, 45453 genes\n",
      " Processed 10000/352734 cells\n",
      " Processed 20000/352734 cells\n",
      " Processed 30000/352734 cells\n",
      " Processed 40000/352734 cells\n",
      " Processed 50000/352734 cells\n",
      " Processed 60000/352734 cells\n",
      " Processed 70000/352734 cells\n",
      " Processed 80000/352734 cells\n",
      " Processed 90000/352734 cells\n",
      " Processed 100000/352734 cells\n",
      " Processed 110000/352734 cells\n",
      " Processed 120000/352734 cells\n",
      " Processed 130000/352734 cells\n",
      " Processed 140000/352734 cells\n",
      " Processed 150000/352734 cells\n",
      " Processed 160000/352734 cells\n",
      " Processed 170000/352734 cells\n",
      " Processed 180000/352734 cells\n",
      " Processed 190000/352734 cells\n",
      " Processed 200000/352734 cells\n",
      " Processed 210000/352734 cells\n",
      " Processed 220000/352734 cells\n",
      " Processed 230000/352734 cells\n",
      " Processed 240000/352734 cells\n",
      " Processed 250000/352734 cells\n",
      " Processed 260000/352734 cells\n",
      " Processed 270000/352734 cells\n",
      " Processed 280000/352734 cells\n",
      " Processed 290000/352734 cells\n",
      " Processed 300000/352734 cells\n",
      " Processed 310000/352734 cells\n",
      " Processed 320000/352734 cells\n",
      " Processed 330000/352734 cells\n",
      " Processed 340000/352734 cells\n",
      " Processed 350000/352734 cells\n",
      " Processed 352734/352734 cells\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to synchronously get dataspace (invalid dataset identifier)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m             filtered_dataset[start:end, :] \u001b[38;5;241m=\u001b[39m filtered_chunk  \u001b[38;5;66;03m# Save chunk\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Filtered dataset saved with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiltered_dataset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m genes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/h5py/_hl/dataset.py:492\u001b[0m, in \u001b[0;36mDataset.shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m phil:\n\u001b[0;32m--> 492\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# If the file is read-only, cache the shape to speed-up future uses.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# This cache is invalidated by .refresh() when using SWMR.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readonly:\n",
      "File \u001b[0;32mh5py/h5d.pyx:191\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.shape.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:192\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.shape.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:348\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.get_space\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to synchronously get dataspace (invalid dataset identifier)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the original HDF5 file\n",
    "with h5py.File(\"tokenized_data.h5\", \"r\") as hf:\n",
    "    total_cells, total_genes = hf[\"tokenized_data\"].shape\n",
    "    print(f\"🔹 Original dataset: {total_cells} cells, {total_genes} genes\")\n",
    "\n",
    "# **Create the new filtered HDF5 file**\n",
    "with h5py.File(\"filtered_tokenized_data.h5\", \"w\") as hf_out:\n",
    "    filtered_dataset = hf_out.create_dataset(\n",
    "        \"tokenized_data\", shape=(total_cells, len(common_gene_indices)), dtype=\"int32\"\n",
    "    )\n",
    "\n",
    "    # **Process in chunks of 10,000 cells**\n",
    "    CHUNK_SIZE = 10000\n",
    "    for start in range(0, total_cells, CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, total_cells)\n",
    "\n",
    "        with h5py.File(\"tokenized_data.h5\", \"r\") as hf_in:\n",
    "            chunk_tokens = hf_in[\"tokenized_data\"][start:end, :]  # Load only small part\n",
    "            filtered_chunk = chunk_tokens[:, common_gene_indices]  # Keep only valid genes\n",
    "            filtered_dataset[start:end, :] = filtered_chunk  # Save chunk\n",
    "\n",
    "        print(f\" Processed {end}/{total_cells} cells\")\n",
    "\n",
    "print(f\" Filtered dataset saved with {filtered_dataset.shape[1]} genes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02e7c2c3-a87f-4c6d-85cf-e1f91b8ca992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original dataset: 352734 cells, 45453 genes\n",
      "Processed 10000/352734 cells\n",
      "Processed 20000/352734 cells\n",
      "Processed 30000/352734 cells\n",
      "Processed 40000/352734 cells\n",
      "Processed 50000/352734 cells\n",
      "Processed 60000/352734 cells\n",
      "Processed 70000/352734 cells\n",
      "Processed 80000/352734 cells\n",
      "Processed 90000/352734 cells\n",
      "Processed 100000/352734 cells\n",
      "Processed 110000/352734 cells\n",
      "Processed 120000/352734 cells\n",
      "Processed 130000/352734 cells\n",
      "Processed 140000/352734 cells\n",
      "Processed 150000/352734 cells\n",
      "Processed 160000/352734 cells\n",
      "Processed 170000/352734 cells\n",
      "Processed 180000/352734 cells\n",
      "Processed 190000/352734 cells\n",
      "Processed 200000/352734 cells\n",
      "Processed 210000/352734 cells\n",
      "Processed 220000/352734 cells\n",
      "Processed 230000/352734 cells\n",
      "Processed 240000/352734 cells\n",
      "Processed 250000/352734 cells\n",
      "Processed 260000/352734 cells\n",
      "Processed 270000/352734 cells\n",
      "Processed 280000/352734 cells\n",
      "Processed 290000/352734 cells\n",
      "Processed 300000/352734 cells\n",
      "Processed 310000/352734 cells\n",
      "Processed 320000/352734 cells\n",
      "Processed 330000/352734 cells\n",
      "Processed 340000/352734 cells\n",
      "Processed 350000/352734 cells\n",
      "Processed 352734/352734 cells\n",
      "Filtered dataset saved with 6705 genes.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the original HDF5 file\n",
    "with h5py.File(\"tokenized_data.h5\", \"r\") as hf:\n",
    "    total_cells, total_genes = hf[\"tokenized_data\"].shape\n",
    "    print(f\" Original dataset: {total_cells} cells, {total_genes} genes\")\n",
    "\n",
    "# **Create the new filtered HDF5 file**\n",
    "with h5py.File(\"filtered_tokenized_data.h5\", \"w\") as hf_out:\n",
    "    filtered_dataset = hf_out.create_dataset(\n",
    "        \"tokenized_data\", shape=(total_cells, len(common_gene_indices)), dtype=\"int32\"\n",
    "    )\n",
    "\n",
    "    # **Process in chunks of 10,000 cells**\n",
    "    CHUNK_SIZE = 10000\n",
    "    for start in range(0, total_cells, CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, total_cells)\n",
    "\n",
    "        with h5py.File(\"tokenized_data.h5\", \"r\") as hf_in:\n",
    "            chunk_tokens = hf_in[\"tokenized_data\"][start:end, :]  # Load only small part\n",
    "            filtered_chunk = chunk_tokens[:, common_gene_indices]  # Keep only valid genes\n",
    "            filtered_dataset[start:end, :] = filtered_chunk  # Save chunk\n",
    "\n",
    "        print(f\"Processed {end}/{total_cells} cells\")\n",
    "\n",
    "    # **Get the shape before closing the file**\n",
    "    filtered_shape = filtered_dataset.shape\n",
    "\n",
    "print(f\"Filtered dataset saved with {filtered_shape[1]} genes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6569b4c-f494-472c-90a7-16e6e46f804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists and contains dataset with shape (352734, 6705)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "file_path = \"filtered_tokenized_data.h5\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with h5py.File(file_path, \"r\") as hf:\n",
    "        if \"tokenized_data\" in hf:\n",
    "            print(f\"File exists and contains dataset with shape {hf['tokenized_data'].shape}\")\n",
    "        else:\n",
    "            print(\" File exists but does not contain 'tokenized_data'.\")\n",
    "else:\n",
    "    print(\" File was not created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe8040db-81c5-4120-9729-269cab7ece2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DataLoader initialized with batch size 5. Ready for inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, tokenized_file, adata):\n",
    "        self.h5_file = tokenized_file\n",
    "        self.num_cells = adata.shape[0]\n",
    "        self.adata = adata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "        \n",
    "        batch_values = torch.tensor(\n",
    "            self.adata.X[idx].todense() if hasattr(self.adata.X, \"todense\") else self.adata.X[idx],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "        src_key_padding_mask = (batch_tokens == pad_index)\n",
    "\n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "# Load filtered dataset\n",
    "dataset = scGPTDataset(\"filtered_tokenized_data.h5\", adata)\n",
    "\n",
    "# Reduce batch size to avoid memory issues\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(f\" DataLoader initialized with batch size {BATCH_SIZE}. Ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db41a60-86cb-4a5b-93c2-c47053c6c524",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6705) must match the size of tensor b (10000) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m src_key_padding_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert to predictions\u001b[39;00m\n\u001b[1;32m     22\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[1;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:184\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    182\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m*\u001b[39m values\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdsbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     batch_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6705) must match the size of tensor b (10000) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Move model to CPU to prevent GPU memory overload\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Store predictions\n",
    "predictions = []\n",
    "\n",
    "# Run inference batch-by-batch\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Convert to predictions\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Free memory after each batch\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\" Inference completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55c56a88-930f-42b6-995d-f3bd67263e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset has 352734 cells and 6705 genes.\n",
      " Found 6705 genes in filtered dataset.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Load the gene indices stored in tokenized_data.h5\n",
    "with h5py.File(\"filtered_tokenized_data.h5\", \"r\") as hf:\n",
    "    total_cells, total_genes = hf[\"tokenized_data\"].shape\n",
    "    print(f\"Filtered dataset has {total_cells} cells and {total_genes} genes.\")\n",
    "\n",
    "    # Extract stored gene indices\n",
    "    stored_gene_indices = list(range(total_genes))  # The 6705 genes kept\n",
    "\n",
    "print(f\" Found {len(stored_gene_indices)} genes in filtered dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a9a1231-5446-4878-8db9-ce53346e35fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated adata to 6705 genes. Ready for inference.\n"
     ]
    }
   ],
   "source": [
    "# Ensure adata contains only the genes in filtered_tokenized_data.h5\n",
    "adata = adata[:, stored_gene_indices]  # Select only the valid genes\n",
    "\n",
    "print(f\" Updated adata to {adata.shape[1]} genes. Ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "323aa50e-6594-43c4-9c06-883376639937",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6705) must match the size of tensor b (10000) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m src_key_padding_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert to predictions\u001b[39;00m\n\u001b[1;32m     22\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[1;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:184\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    182\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m*\u001b[39m values\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdsbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     batch_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6705) must match the size of tensor b (10000) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Run inference batch-by-batch\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)  # Now has correct shape (6705)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Convert to predictions\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Free memory after each batch\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\" Inference completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c220a5eb-ed67-4bf2-affe-cf4fcbdceffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated `adata` to 6705 genes (should be 6705).\n"
     ]
    }
   ],
   "source": [
    "# Get gene names in `adata`\n",
    "adata_gene_names = list(adata.var[\"feature_name\"])\n",
    "\n",
    "# Ensure `adata` only keeps genes that exist in `filtered_tokenized_data.h5`\n",
    "filtered_gene_names = adata_gene_names[:6705]  # Select the first 6705 genes\n",
    "adata = adata[:, adata.var[\"feature_name\"].isin(filtered_gene_names)]\n",
    "\n",
    "print(f\" Updated `adata` to {adata.shape[1]} genes (should be 6705).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ec3a31-6e09-482f-b231-67e67839ddef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " `adata.X` shape: (352734, 45453)\n",
      " `filtered_tokenized_data.h5` shape: (352734, 6705)\n"
     ]
    }
   ],
   "source": [
    "# Ensure adata.X and tokenized dataset have the same shape\n",
    "with h5py.File(\"filtered_tokenized_data.h5\", \"r\") as hf:\n",
    "    total_cells, total_genes = hf[\"tokenized_data\"].shape\n",
    "\n",
    "print(f\" `adata.X` shape: {adata.shape}\")\n",
    "print(f\" `filtered_tokenized_data.h5` shape: ({total_cells}, {total_genes})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15d26631-9785-47d7-8723-2b8517410c4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6705) must match the size of tensor b (10000) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m src_key_padding_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert to predictions\u001b[39;00m\n\u001b[1;32m     22\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[1;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:184\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    182\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m*\u001b[39m values\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdsbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     batch_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6705) must match the size of tensor b (10000) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Move model to CPU (if you need GPU, change \"cpu\" to \"cuda\")\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Run inference batch-by-batch\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Convert to predictions\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Free memory after each batch\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\" Inference completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c25f051-3895-4957-8fa4-aeee265aaadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_tokens shape: torch.Size([5, 6705])\n",
      "batch_values shape: torch.Size([5, 1, 10000])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape Mismatch! batch_tokens: torch.Size([5, 6705]), batch_values: torch.Size([5, 1, 10000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_values shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Expected: (batch_size, 6705)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m batch_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape Mismatch! batch_tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_tokens\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch_values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m     26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_tokens, batch_values, src_key_padding_mask)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape Mismatch! batch_tokens: torch.Size([5, 6705]), batch_values: torch.Size([5, 1, 10000])"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Move model to CPU (if you need GPU, change \"cpu\" to \"cuda\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Run inference batch-by-batch\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # **Print actual tensor shapes before running inference**\n",
    "        print(f\"batch_tokens shape: {batch_tokens.shape}\")  # Expected: (batch_size, 6705)\n",
    "        print(f\"batch_values shape: {batch_values.shape}\")  # Expected: (batch_size, 6705)\n",
    "        \n",
    "        if batch_tokens.shape[1] != batch_values.shape[1]:\n",
    "            raise ValueError(f\"Shape Mismatch! batch_tokens: {batch_tokens.shape}, batch_values: {batch_values.shape}\")\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Convert to predictions\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Free memory after each batch\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\" Inference completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63d2e164-1a64-4bc9-9f33-84535278565f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoader reinitialized. Run inference again.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, tokenized_file, adata):\n",
    "        self.h5_file = tokenized_file\n",
    "        self.num_cells = adata.shape[0]\n",
    "        self.adata = adata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "\n",
    "        # **Convert Sparse to Dense**\n",
    "        batch_values_dense = self.adata.X[idx].todense() if hasattr(self.adata.X, \"todense\") else self.adata.X[idx]\n",
    "        batch_values_dense = np.asarray(batch_values_dense)  # Ensure NumPy array\n",
    "        batch_values = torch.tensor(batch_values_dense.squeeze(), dtype=torch.float32)\n",
    "\n",
    "        # **Check final shapes**\n",
    "        print(f\"Final batch_tokens shape: {batch_tokens.shape}\")  # Expected: (batch_size, 6705)\n",
    "        print(f\"Final batch_values shape: {batch_values.shape}\")  # Expected: (batch_size, 6705)\n",
    "\n",
    "        pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "        src_key_padding_mask = (batch_tokens == pad_index)\n",
    "\n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "# **Reinitialize DataLoader to apply the fix**\n",
    "BATCH_SIZE = 5\n",
    "dataloader = DataLoader(scGPTDataset(\"filtered_tokenized_data.h5\", adata), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"✅ DataLoader reinitialized. Run inference again.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fca9059-5e75-473a-ae1e-bcab2cabb821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final batch_tokens shape: torch.Size([6705])\n",
      "Final batch_values shape: torch.Size([6705])\n",
      "Final batch_tokens shape: torch.Size([6705])\n",
      "Final batch_values shape: torch.Size([6705])\n",
      "Final batch_tokens shape: torch.Size([6705])\n",
      "Final batch_values shape: torch.Size([6705])\n",
      "Final batch_tokens shape: torch.Size([6705])\n",
      "Final batch_values shape: torch.Size([6705])\n",
      "Final batch_tokens shape: torch.Size([6705])\n",
      "Final batch_values shape: torch.Size([6705])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_tokens, batch_values, src_key_padding_mask)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert to predictions\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     23\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(batch_predictions)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Free memory after each batch\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not dict"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Move model to CPU (or use \"cuda\" for GPU)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Run inference batch-by-batch\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)  # Now correctly (batch_size, 6705)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Convert to predictions\n",
    "        batch_predictions = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Free memory after each batch\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"✅ Inference completed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "982d43ca-078e-4ccf-bc9b-c3ca241a5759",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_tokens, batch_values, src_key_padding_mask \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m:\n\u001b[1;32m      8\u001b[0m         batch_tokens \u001b[38;5;241m=\u001b[39m batch_tokens\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m         batch_values \u001b[38;5;241m=\u001b[39m batch_values\u001b[38;5;241m.\u001b[39mto(device)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Run inference batch-by-batch\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)  \n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # **Print the type and keys of `outputs`**\n",
    "        print(f\"Outputs type: {type(outputs)}\")\n",
    "        if isinstance(outputs, dict):\n",
    "            print(f\"Outputs keys: {outputs.keys()}\")\n",
    "        else:\n",
    "            print(f\"Outputs is not a dictionary, shape: {outputs.shape}\")\n",
    "\n",
    "        break  # Stop after first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac15989-e7b5-4d31-879d-5e613f389a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, tokenized_file, adata):\n",
    "        self.h5_file = tokenized_file\n",
    "        self.num_cells = adata.shape[0]\n",
    "        self.adata = adata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "\n",
    "        # Convert Sparse Matrix to Dense\n",
    "        batch_values_dense = self.adata.X[idx].todense() if hasattr(self.adata.X, \"todense\") else self.adata.X[idx]\n",
    "        batch_values_dense = np.asarray(batch_values_dense)  # Ensure NumPy array\n",
    "        batch_values = torch.tensor(batch_values_dense.squeeze(), dtype=torch.float32)\n",
    "\n",
    "        pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "        src_key_padding_mask = (batch_tokens == pad_index)\n",
    "\n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32747b50-8d6a-4b2a-bfe9-6a60d16840ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoader reinitialized. You can now run inference.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "dataloader = DataLoader(scGPTDataset(\"filtered_tokenized_data.h5\", adata), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"✅ DataLoader reinitialized. You can now run inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d014a932-5cea-4d1b-bce3-734a3afb32cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'predictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputs keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outputs \u001b[38;5;28;01melse\u001b[39;00m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m outputs\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predictions'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Run inference batch-by-batch\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)  # Now correctly (batch_size, 6705)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Print output structure\n",
    "        print(f\"Outputs type: {type(outputs)}\")\n",
    "        if isinstance(outputs, dict):\n",
    "            print(f\"Outputs keys: {outputs.keys()}\")\n",
    "            output_tensor = outputs[\"logits\"] if \"logits\" in outputs else outputs[\"predictions\"]\n",
    "        else:\n",
    "            output_tensor = outputs\n",
    "\n",
    "        # Convert to predictions\n",
    "        batch_predictions = torch.argmax(output_tensor, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Free memory\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Inference completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cc894-9883-42e9-9c0e-244518553e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n",
      "Outputs type: <class 'dict'>\n",
      "Outputs keys: dict_keys(['mlm_output', 'cell_emb'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "# Ensure correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the dataset class for scGPT\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, tokenized_file, adata):\n",
    "        self.h5_file = tokenized_file\n",
    "        self.num_cells = adata.shape[0]\n",
    "        self.adata = adata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "\n",
    "        # Convert Sparse Matrix to Dense\n",
    "        batch_values_dense = self.adata.X[idx].todense() if hasattr(self.adata.X, \"todense\") else self.adata.X[idx]\n",
    "        batch_values_dense = np.asarray(batch_values_dense)  # Ensure NumPy array\n",
    "        batch_values = torch.tensor(batch_values_dense.squeeze(), dtype=torch.float32)\n",
    "\n",
    "        # Create padding mask\n",
    "        pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "        src_key_padding_mask = (batch_tokens == pad_index)\n",
    "\n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "# Initialize DataLoader\n",
    "BATCH_SIZE = 5\n",
    "dataloader = DataLoader(scGPTDataset(\"filtered_tokenized_data.h5\", adata), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Store predictions\n",
    "predictions = []\n",
    "\n",
    "# Run inference batch-by-batch\n",
    "with torch.no_grad():\n",
    "    for batch_tokens, batch_values, src_key_padding_mask in dataloader:\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)  \n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # Print output structure\n",
    "        print(f\"Outputs type: {type(outputs)}\")\n",
    "        print(f\"Outputs keys: {outputs.keys()}\")  # Check available keys\n",
    "\n",
    "        # Extract correct tensor from `outputs`\n",
    "        if isinstance(outputs, dict):\n",
    "            if \"mlm_output\" in outputs:\n",
    "                output_tensor = outputs[\"mlm_output\"]  # Use this for predictions\n",
    "            elif \"cell_emb\" in outputs:\n",
    "                output_tensor = outputs[\"cell_emb\"]  # Alternative: Use cell embeddings\n",
    "            else:\n",
    "                raise KeyError(f\"Expected tensor key not found in outputs: {outputs.keys()}\")\n",
    "        else:\n",
    "            output_tensor = outputs  # If outputs is already a tensor\n",
    "\n",
    "        # Convert to predictions\n",
    "        batch_predictions = torch.argmax(output_tensor, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Free memory\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"✅ Inference completed successfully!\")\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "\n",
    "# Save predictions\n",
    "np.save(\"scGPT_predictions.npy\", np.array(predictions))\n",
    "print(\"✅ Predictions saved as 'scGPT_predictions.npy'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8703d7c-722b-4636-a5f7-050c7f5706c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import json\n",
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import GeneVocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7902821-300f-47bc-af6b-c2207997923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "# Ensure correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the dataset class for scGPT\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, tokenized_file, adata):\n",
    "        self.h5_file = tokenized_file\n",
    "        self.num_cells = adata.shape[0]\n",
    "        self.adata = adata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "\n",
    "        # Convert Sparse Matrix to Dense\n",
    "        batch_values_dense = self.adata.X[idx].todense() if hasattr(self.adata.X, \"todense\") else self.adata.X[idx]\n",
    "        batch_values_dense = np.asarray(batch_values_dense)  # Ensure NumPy array\n",
    "        batch_values = torch.tensor(batch_values_dense.squeeze(), dtype=torch.float32)\n",
    "\n",
    "        # Create padding mask\n",
    "        pad_index = vocab.get_stoi().get(\"<PAD>\", None)\n",
    "        src_key_padding_mask = (batch_tokens == pad_index)\n",
    "\n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a70d9b-6850-4fcd-88f7-d3924e1dcd75",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6705) must match the size of tensor b (10000) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m src_key_padding_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlm_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[1;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scgpt/model/model.py:184\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[1;32m    182\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m*\u001b[39m values\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdsbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     batch_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6705) must match the size of tensor b (10000) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Increase batch size\n",
    "BATCH_SIZE = 128  # Try 128 or 256 if memory allows\n",
    "dataloader = DataLoader(scGPTDataset(\"filtered_tokenized_data.h5\", adata), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Run inference\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (batch_tokens, batch_values, src_key_padding_mask) in enumerate(dataloader):\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # Run model\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "        output_tensor = outputs[\"mlm_output\"]\n",
    "\n",
    "        # Convert predictions\n",
    "        batch_predictions = torch.argmax(output_tensor, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # Print progress every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i * BATCH_SIZE}/{len(dataloader.dataset)} cells\")\n",
    "\n",
    "        # Free memory\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "print(\" Inference completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00c4bfa-c850-4634-adda-5f97f9ad7c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded vocab with 60697 genes.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'ntokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#  Initialize Model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mntokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_hid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_hid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#  Load Model Weights\u001b[39;00m\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_file, map_location\u001b[38;5;241m=\u001b[39mdevice))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'ntokens'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import json\n",
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import GeneVocab\n",
    "\n",
    "#  Load Model and Data Paths\n",
    "model_dir = Path(\"/home/master/Documents/scgpt/\")\n",
    "model_config_file = model_dir / \"args.json\"\n",
    "model_file = model_dir / \"best_model.pt\"\n",
    "vocab_file = model_dir / \"vocab.json\"\n",
    "h5_file = \"filtered_tokenized_data.h5\"\n",
    "\n",
    "#  Load Vocabulary\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "print(f\" Loaded vocab with {len(vocab)} genes.\")\n",
    "\n",
    "#  Load Model Config\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "\n",
    "#  Model Parameters\n",
    "embsize = model_configs.get(\"embsize\", 512)\n",
    "nhead = model_configs.get(\"nheads\", 8)\n",
    "d_hid = model_configs.get(\"d_hid\", 2048)\n",
    "nlayers = model_configs.get(\"nlayers\", 12)\n",
    "\n",
    "#  Initialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(\n",
    "    ntokens=len(vocab),\n",
    "    d_model=embsize,\n",
    "    nhead=nhead,\n",
    "    d_hid=d_hid,\n",
    "    nlayers=nlayers,\n",
    "    vocab=vocab,\n",
    "    pad_value=0\n",
    ").to(device)\n",
    "\n",
    "#  Load Model Weights\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "model.eval()\n",
    "print(\" scGPT Model Loaded on\", device)\n",
    "\n",
    "#  Load Processed Data\n",
    "adata = sc.read_h5ad(\"filtered_adata.h5ad\")  # Ensure genes are pre-filtered\n",
    "print(f\" Loaded AnnData: {adata.shape[0]} cells, {adata.shape[1]} genes.\")\n",
    "\n",
    "#  Filter common genes between vocab and dataset\n",
    "common_genes = [gene for gene in adata.var_names if gene in vocab.get_stoi()]\n",
    "print(f\" Found {len(common_genes)} common genes between dataset and vocab.\")\n",
    "\n",
    "#  Reduce dataset to common genes\n",
    "adata = adata[:, common_genes]\n",
    "print(f\" Filtered dataset now has {adata.shape[1]} genes.\")\n",
    "\n",
    "#  Define Dataset Class\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, h5_file, adata, common_genes):\n",
    "        self.h5_file = h5_file\n",
    "        self.adata = adata\n",
    "        self.common_gene_indices = [list(adata.var_names).index(g) for g in common_genes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.adata.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "\n",
    "        batch_values = torch.tensor(\n",
    "            self.adata.X[idx, self.common_gene_indices].todense() \n",
    "            if hasattr(self.adata.X, \"todense\") \n",
    "            else self.adata.X[idx, self.common_gene_indices],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        src_key_padding_mask = batch_tokens == 0  # Assume padding tokens are 0\n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "#  Create Dataset & DataLoader\n",
    "BATCH_SIZE = 128\n",
    "dataset = scGPTDataset(h5_file, adata, common_genes)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\" Dataset Loaded: {len(dataset)} cells, {adata.shape[1]} genes.\")\n",
    "\n",
    "#  Run Inference Efficiently\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (batch_tokens, batch_values, src_key_padding_mask) in enumerate(dataloader):\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        #  Debug Shapes Before Inference\n",
    "        print(f\"Batch {i}: batch_tokens shape: {batch_tokens.shape}, batch_values shape: {batch_values.shape}\")\n",
    "\n",
    "        #  Run Model\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        #  Extract Predictions\n",
    "        if isinstance(outputs, dict) and \"mlm_output\" in outputs:\n",
    "            output_tensor = outputs[\"mlm_output\"]\n",
    "        else:\n",
    "            raise KeyError(\"Model output does not contain 'mlm_output' key!\")\n",
    "\n",
    "        batch_predictions = torch.argmax(output_tensor, dim=1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        #  Print Progress Every 100 Batches\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i * BATCH_SIZE}/{len(dataloader.dataset)} cells\")\n",
    "\n",
    "        #  Free Memory\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\" Inference Completed!\")\n",
    "\n",
    "#  Save Predictions\n",
    "import numpy as np\n",
    "np.save(\"scGPT_predictions.npy\", predictions)\n",
    "print(\"Predictions Saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ce8505-e4ae-4ddf-b59a-9fd46cb77fed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'PWAR1' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# ** Load Processed Data **\u001b[39;00m\n\u001b[1;32m     41\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m---> 42\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mscGPTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiltered_tokenized_data.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_genes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Dataset Loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m genes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36mscGPTDataset.__init__\u001b[0;34m(self, h5_file, adata, common_genes)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_file \u001b[38;5;241m=\u001b[39m h5_file\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata \u001b[38;5;241m=\u001b[39m adata\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommon_gene_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(adata\u001b[38;5;241m.\u001b[39mvar_names)\u001b[38;5;241m.\u001b[39mindex(g) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m common_genes]\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_file \u001b[38;5;241m=\u001b[39m h5_file\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata \u001b[38;5;241m=\u001b[39m adata\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommon_gene_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_names\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m common_genes]\n",
      "\u001b[0;31mValueError\u001b[0m: 'PWAR1' is not in list"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Ensure model is already loaded from previous execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# ** Define Dataset Class **\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, h5_file, adata, common_genes):\n",
    "        self.h5_file = h5_file\n",
    "        self.adata = adata\n",
    "        self.common_gene_indices = [list(adata.var_names).index(g) for g in common_genes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.adata.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "\n",
    "        # Ensure values are correctly shaped\n",
    "        batch_values = self.adata.X[idx, self.common_gene_indices]\n",
    "        if hasattr(batch_values, \"todense\"):  # Convert sparse to dense if necessary\n",
    "            batch_values = batch_values.todense()\n",
    "\n",
    "        batch_values = torch.tensor(batch_values, dtype=torch.float32).squeeze()\n",
    "\n",
    "        # Ensure dimensions match model expectation\n",
    "        if batch_values.ndim == 1:\n",
    "            batch_values = batch_values.unsqueeze(0)  # Convert (num_genes,) -> (1, num_genes)\n",
    "\n",
    "        # Create padding mask\n",
    "        src_key_padding_mask = batch_tokens == 0  # Assuming 0 is padding token\n",
    "\n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "# ** Load Processed Data **\n",
    "BATCH_SIZE = 128\n",
    "dataset = scGPTDataset(\"filtered_tokenized_data.h5\", adata, common_genes)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\" Dataset Loaded: {len(dataset)} cells, {adata.shape[1]} genes.\")\n",
    "\n",
    "# ** Run Inference **\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (batch_tokens, batch_values, src_key_padding_mask) in enumerate(dataloader):\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # ** Debug: Check Tensor Shapes **\n",
    "        print(f\"Batch {i}: batch_tokens shape: {batch_tokens.shape}, batch_values shape: {batch_values.shape}\")\n",
    "\n",
    "        # ** Run Model **\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # ** Fix Output Parsing **\n",
    "        if isinstance(outputs, dict) and \"mlm_output\" in outputs:\n",
    "            output_tensor = outputs[\"mlm_output\"]  # Correct key for predictions\n",
    "        else:\n",
    "            raise KeyError(\"Model output does not contain 'mlm_output' key!\")\n",
    "\n",
    "        batch_predictions = torch.argmax(output_tensor, dim=-1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # ** Free Memory **\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # ** Progress Update Every 100 Batches **\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i * BATCH_SIZE}/{len(dataloader.dataset)} cells\")\n",
    "\n",
    "print(\" Inference Completed!\")\n",
    "\n",
    "# ** Save Predictions to File **\n",
    "np.save(\"scGPT_predictions.npy\", predictions)\n",
    "print(\" Predictions Saved Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3b1e5ea-d702-4dc8-83fb-ab0742d430e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 0/6705 genes in adata.\n",
      "✅ Dataset Loaded: 352734 cells, 10000 genes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_tokens, batch_values, src_key_padding_mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     57\u001b[0m         batch_tokens \u001b[38;5;241m=\u001b[39m batch_tokens\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m         batch_values \u001b[38;5;241m=\u001b[39m batch_values\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m, in \u001b[0;36mscGPTDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(hf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_data\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Ensure values are correctly shaped\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m batch_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m[idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommon_gene_indices]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(batch_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtodense\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Convert sparse to dense if necessary\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     batch_values \u001b[38;5;241m=\u001b[39m batch_values\u001b[38;5;241m.\u001b[39mtodense()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/anndata/_core/anndata.py:562\u001b[0m, in \u001b[0;36mAnnData.X\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_view:\n\u001b[1;32m    561\u001b[0m     X \u001b[38;5;241m=\u001b[39m as_view(\n\u001b[0;32m--> 562\u001b[0m         \u001b[43m_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adata_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vidx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    563\u001b[0m         ElementRef(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    564\u001b[0m     )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/functools.py:888\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    886\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/anndata/_core/index.py:168\u001b[0m, in \u001b[0;36m_subset_spmatrix\u001b[0;34m(a, subset_idx)\u001b[0m\n\u001b[1;32m    166\u001b[0m         first_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(first_idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    167\u001b[0m     subset_idx \u001b[38;5;241m=\u001b[39m (first_idx\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m*\u001b[39msubset_idx[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubset_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scipy/sparse/_index.py:76\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sliceXslice(row, col)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_sliceXarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex results in >2 dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m row\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scipy/sparse/_csr.py:208\u001b[0m, in \u001b[0;36m_csr_base._get_sliceXarray\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_sliceXarray\u001b[39m(\u001b[38;5;28mself\u001b[39m, row, col):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_major_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_minor_index_fancy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/scipy/sparse/_compressed.py:797\u001b[0m, in \u001b[0;36m_cs_matrix._minor_index_fancy\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    795\u001b[0m res_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(nnz, dtype\u001b[38;5;241m=\u001b[39midx_dtype)\n\u001b[1;32m    796\u001b[0m res_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(nnz, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 797\u001b[0m \u001b[43mcsr_column_index2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_offsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m((res_data, res_indices, res_indptr),\n\u001b[1;32m    800\u001b[0m                       shape\u001b[38;5;241m=\u001b[39mnew_shape, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Ensure model is already loaded from previous execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# ** Define Dataset Class **\n",
    "class scGPTDataset(Dataset):\n",
    "    def __init__(self, h5_file, adata, common_genes):\n",
    "        self.h5_file = h5_file\n",
    "        self.adata = adata\n",
    "        \n",
    "        # Ensure we only keep genes that exist in adata\n",
    "        self.common_gene_indices = [\n",
    "            list(adata.var_names).index(g) for g in common_genes if g in adata.var_names\n",
    "        ]\n",
    "\n",
    "        print(f\"✅ Found {len(self.common_gene_indices)}/{len(common_genes)} genes in adata.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.adata.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as hf:\n",
    "            batch_tokens = torch.tensor(hf[\"tokenized_data\"][idx], dtype=torch.long)\n",
    "\n",
    "        # Ensure values are correctly shaped\n",
    "        batch_values = self.adata.X[idx, self.common_gene_indices]\n",
    "        if hasattr(batch_values, \"todense\"):  # Convert sparse to dense if necessary\n",
    "            batch_values = batch_values.todense()\n",
    "\n",
    "        batch_values = torch.tensor(batch_values, dtype=torch.float32).squeeze()\n",
    "\n",
    "        # Ensure dimensions match model expectation\n",
    "        if batch_values.ndim == 1:\n",
    "            batch_values = batch_values.unsqueeze(0)  # Convert (num_genes,) -> (1, num_genes)\n",
    "\n",
    "        # Create padding mask\n",
    "        src_key_padding_mask = batch_tokens == 0  # Assuming 0 is padding token\n",
    "\n",
    "        return batch_tokens, batch_values, src_key_padding_mask\n",
    "\n",
    "# ** Load Processed Data **\n",
    "BATCH_SIZE = 128\n",
    "dataset = scGPTDataset(\"filtered_tokenized_data.h5\", adata, common_genes)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"✅ Dataset Loaded: {len(dataset)} cells, {adata.shape[1]} genes.\")\n",
    "\n",
    "# ** Run Inference **\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (batch_tokens, batch_values, src_key_padding_mask) in enumerate(dataloader):\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_values = batch_values.to(device)\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "\n",
    "        # ** Debug: Check Tensor Shapes **\n",
    "        print(f\"Batch {i}: batch_tokens shape: {batch_tokens.shape}, batch_values shape: {batch_values.shape}\")\n",
    "\n",
    "        # ** Run Model **\n",
    "        outputs = model(batch_tokens, batch_values, src_key_padding_mask)\n",
    "\n",
    "        # ** Fix Output Parsing **\n",
    "        if isinstance(outputs, dict) and \"mlm_output\" in outputs:\n",
    "            output_tensor = outputs[\"mlm_output\"]  # Correct key for predictions\n",
    "        else:\n",
    "            raise KeyError(\"Model output does not contain 'mlm_output' key!\")\n",
    "\n",
    "        batch_predictions = torch.argmax(output_tensor, dim=-1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "        # ** Free Memory **\n",
    "        del batch_tokens, batch_values, src_key_padding_mask, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # ** Progress Update Every 100 Batches **\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i * BATCH_SIZE}/{len(dataloader.dataset)} cells\")\n",
    "\n",
    "print(\"✅ Inference Completed!\")\n",
    "\n",
    "# ** Save Predictions to File **\n",
    "np.save(\"scGPT_predictions.npy\", predictions)\n",
    "print(\"✅ Predictions Saved Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180e5ee0-c640-4e78-9d08-d0d32059948a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ENSG00000000003', 'ENSG00000000005', 'ENSG00000000419',\n",
      "       'ENSG00000000457', 'ENSG00000000460', 'ENSG00000000938',\n",
      "       'ENSG00000000971', 'ENSG00000001036', 'ENSG00000001084',\n",
      "       'ENSG00000001167'],\n",
      "      dtype='object', name='index')\n"
     ]
    }
   ],
   "source": [
    "print(adata.var_names[:10])  # Check first few gene names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb4e2e6-a20c-4a2d-8489-fa6c56925ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'common_genes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcommon_genes\u001b[49m[:\u001b[38;5;241m10\u001b[39m])  \u001b[38;5;66;03m# Check first few common genes\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'common_genes' is not defined"
     ]
    }
   ],
   "source": [
    "print(common_genes[:10])  # Check first few common genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aff8d94-c9ad-4cec-9a34-55d7f33a86f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 0 common genes between adata and vocab.\n"
     ]
    }
   ],
   "source": [
    "# Ensure gene names are in the correct format\n",
    "adata_gene_names = list(adata.var_names)  # Convert to list\n",
    "vocab_gene_names = list(vocab.get_stoi().keys())  # Get vocab gene names\n",
    "\n",
    "# Find common genes\n",
    "common_genes = list(set(adata_gene_names) & set(vocab_gene_names))\n",
    "\n",
    "print(f\"✅ Found {len(common_genes)} common genes between adata and vocab.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c6bd0a-4f15-4356-9fda-ddb574516f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 genes in adata: ['ENSG00000000003', 'ENSG00000000005', 'ENSG00000000419', 'ENSG00000000457', 'ENSG00000000460', 'ENSG00000000938', 'ENSG00000000971', 'ENSG00000001036', 'ENSG00000001084', 'ENSG00000001167']\n",
      "First 10 genes in vocab: ['<UNK>', 'RP5-973N23.5', 'RP11-182N22.10', 'CTB-53D8.3', 'RP11-348N17.2', 'RP11-205M20.8', 'RP11-326C3.17', 'RP11-439H13.3', 'RP11-413H22.3', 'GET1-SH3BGR']\n"
     ]
    }
   ],
   "source": [
    "# Convert adata.var_names to a proper list\n",
    "adata_gene_names = list(adata.var_names.astype(str))  # Ensure it's a string list\n",
    "\n",
    "# Extract gene names from vocab\n",
    "vocab_gene_names = list(vocab.get_stoi().keys())  # Get vocab dictionary keys\n",
    "\n",
    "# Check sample genes before normalization\n",
    "print(f\"First 10 genes in adata: {adata_gene_names[:10]}\")\n",
    "print(f\"First 10 genes in vocab: {vocab_gene_names[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b4a9514-b0f9-4bd5-9b8c-85ace6f40054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34718 common genes between adata and vocab.\n"
     ]
    }
   ],
   "source": [
    "# Convert Ensembl IDs in adata to gene symbols using 'feature_name'\n",
    "adata_gene_names = list(adata.var[\"feature_name\"].astype(str))  # Convert to gene symbols\n",
    "\n",
    "# Standardize formatting: uppercase, no extra spaces\n",
    "adata_gene_names = [g.upper().strip() for g in adata_gene_names]\n",
    "vocab_gene_names = [g.upper().strip() for g in vocab.get_stoi().keys()]\n",
    "\n",
    "# Find common genes\n",
    "common_genes = list(set(adata_gene_names) & set(vocab_gene_names))\n",
    "\n",
    "# Print results\n",
    "print(f\"Found {len(common_genes)} common genes between adata and vocab.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022e5de4-8d05-43de-b019-548ac050da27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25983 genes in vocab not found in adata.\n",
      " 10735 genes in adata not found in vocab.\n",
      " Sample missing genes from vocab: ['<UNK>', 'RP5-973N23.5', 'RP11-182N22.10', 'CTB-53D8.3', 'RP11-348N17.2', 'RP11-205M20.8', 'RP11-326C3.17', 'RP11-439H13.3', 'RP11-413H22.3', 'GET1-SH3BGR']\n",
      " Sample missing genes from adata: ['ATOSB', 'BLTP2', 'METTL13', 'LDAF1', 'ZNF285CP', 'BMAL2', 'CLXN', 'C6_ENSG00000039537', 'C2ORF83_ENSG00000042304', 'ATOSA']\n"
     ]
    }
   ],
   "source": [
    "missing_from_adata = [g for g in vocab_gene_names if g not in adata_gene_names]\n",
    "missing_from_vocab = [g for g in adata_gene_names if g not in vocab_gene_names]\n",
    "\n",
    "print(f\" {len(missing_from_adata)} genes in vocab not found in adata.\")\n",
    "print(f\" {len(missing_from_vocab)} genes in adata not found in vocab.\")\n",
    "\n",
    "print(f\" Sample missing genes from vocab: {missing_from_adata[:10]}\")\n",
    "print(f\" Sample missing genes from adata: {missing_from_vocab[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "628a6a45-35ec-49d2-bab0-91e73f5ef3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract gene symbol (before \"_ENSG...\") if present\n",
    "adata_gene_names_cleaned = [re.sub(r\"_ENSG\\d+.*\", \"\", g) for g in adata_gene_names]\n",
    "\n",
    "# Convert to uppercase for uniform comparison\n",
    "adata_gene_names_cleaned = [g.upper().strip() for g in adata_gene_names_cleaned]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "754d3a49-7b24-47c1-9cfc-9ec3463580e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 34881 common genes between adata and vocab.\n"
     ]
    }
   ],
   "source": [
    "# Ensure vocab gene names are also in uppercase and cleaned\n",
    "vocab_gene_names = [g.upper().strip() for g in vocab.get_stoi().keys()]\n",
    "\n",
    "# Find common genes again after cleaning\n",
    "common_genes = list(set(adata_gene_names_cleaned) & set(vocab_gene_names))\n",
    "\n",
    "print(f\" Found {len(common_genes)} common genes between adata and vocab.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a9d0b6-a445-4363-a7c7-35b9249494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25820 genes in vocab not found in adata.\n",
      "10571 genes in adata not found in vocab.\n",
      " Sample missing genes from vocab: ['<UNK>', 'RP5-973N23.5', 'RP11-182N22.10', 'CTB-53D8.3', 'RP11-348N17.2', 'RP11-205M20.8', 'RP11-326C3.17', 'RP11-439H13.3', 'RP11-413H22.3', 'GET1-SH3BGR']\n",
      " Sample missing genes from adata: ['ATOSB', 'BLTP2', 'METTL13', 'LDAF1', 'MATR3', 'ZNF285CP', 'BMAL2', 'CLXN', 'ATOSA', 'FHIP1B']\n"
     ]
    }
   ],
   "source": [
    "missing_from_adata = [g for g in vocab_gene_names if g not in adata_gene_names_cleaned]\n",
    "missing_from_vocab = [g for g in adata_gene_names_cleaned if g not in vocab_gene_names]\n",
    "\n",
    "print(f\" {len(missing_from_adata)} genes in vocab not found in adata.\")\n",
    "print(f\"{len(missing_from_vocab)} genes in adata not found in vocab.\")\n",
    "\n",
    "print(f\" Sample missing genes from vocab: {missing_from_adata[:10]}\")\n",
    "print(f\" Sample missing genes from adata: {missing_from_vocab[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d9c81-e72a-4205-ac37-84c912252516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
